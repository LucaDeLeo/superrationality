# Model Registry - Central configuration for all available models
# Add new models here without changing any code!

models:
  # ==================== OpenAI Models ====================
  gpt-4o:
    provider: openai
    model_id: "openai/gpt-4o"
    display_name: "GPT-4 Optimized"
    category: large
    capabilities:
      - reasoning
      - coding
      - general
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 90000
    estimated_cost:
      input_per_1k: 0.005
      output_per_1k: 0.015

  gpt-4o-mini:
    provider: openai
    model_id: "openai/gpt-4o-mini"
    display_name: "GPT-4 Mini"
    category: small
    capabilities:
      - general
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 100
      tokens_per_minute: 150000
    estimated_cost:
      input_per_1k: 0.00015
      output_per_1k: 0.0006

  o1:
    provider: openai
    model_id: "openai/o1"
    display_name: "O1 Reasoning"
    category: reasoning
    capabilities:
      - deep-reasoning
      - complex-tasks
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 30
      tokens_per_minute: 60000
    estimated_cost:
      input_per_1k: 0.015
      output_per_1k: 0.060

  # ==================== Anthropic Models ====================
  claude-4-opus:
    provider: anthropic
    model_id: "anthropic/claude-4-opus"
    display_name: "Claude 4 Opus"
    category: large
    capabilities:
      - reasoning
      - coding
      - extended-thinking
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 40
      tokens_per_minute: 60000
    estimated_cost:
      input_per_1k: 0.015
      output_per_1k: 0.075

  claude-4-sonnet:
    provider: anthropic
    model_id: "anthropic/claude-4-sonnet"
    display_name: "Claude 4 Sonnet"
    category: medium
    capabilities:
      - reasoning
      - coding
      - extended-thinking
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 50
      tokens_per_minute: 80000
    estimated_cost:
      input_per_1k: 0.003
      output_per_1k: 0.015

  claude-3.5-sonnet:
    provider: anthropic
    model_id: "anthropic/claude-3.5-sonnet"
    display_name: "Claude 3.5 Sonnet"
    category: medium
    capabilities:
      - reasoning
      - coding
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 50
      tokens_per_minute: 80000
    estimated_cost:
      input_per_1k: 0.003
      output_per_1k: 0.015

  claude-3-haiku:
    provider: anthropic
    model_id: "anthropic/claude-3-haiku"
    display_name: "Claude 3 Haiku"
    category: small
    capabilities:
      - general
      - fast
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 100
      tokens_per_minute: 100000
    estimated_cost:
      input_per_1k: 0.00025
      output_per_1k: 0.00125

  # ==================== Google Models ====================
  gemini-2.5-pro:
    provider: google
    model_id: "google/gemini-2.5-pro"
    display_name: "Gemini 2.5 Pro"
    category: large
    capabilities:
      - reasoning
      - multimodal
      - deep-think
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 100000
    estimated_cost:
      input_per_1k: 0.00125
      output_per_1k: 0.005

  gemini-2.0-flash:
    provider: google
    model_id: "google/gemini-2.0-flash"
    display_name: "Gemini 2.0 Flash"
    category: medium
    capabilities:
      - multimodal
      - fast
      - tool-use
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 100
      tokens_per_minute: 150000
    estimated_cost:
      input_per_1k: 0.00025
      output_per_1k: 0.001

  gemini-1.5-flash:
    provider: google
    model_id: "google/gemini-1.5-flash"
    display_name: "Gemini 1.5 Flash"
    category: small
    capabilities:
      - general
      - fast
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 100
      tokens_per_minute: 150000
    estimated_cost:
      input_per_1k: 0.0001
      output_per_1k: 0.0004

  # ==================== DeepSeek Models ====================
  deepseek-r1:
    provider: deepseek
    model_id: "deepseek/deepseek-r1"
    display_name: "DeepSeek R1"
    category: large
    capabilities:
      - reasoning
      - coding
      - open-source
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 30
      tokens_per_minute: 50000
    estimated_cost:
      input_per_1k: 0.0001
      output_per_1k: 0.0002

  deepseek-v3:
    provider: deepseek
    model_id: "deepseek/deepseek-v3"
    display_name: "DeepSeek V3"
    category: large
    capabilities:
      - general
      - coding
      - open-source
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 40
      tokens_per_minute: 60000
    estimated_cost:
      input_per_1k: 0.0001
      output_per_1k: 0.0002

  deepseek-r1t-chimera:
    provider: deepseek
    model_id: "deepseek/deepseek-r1t-chimera"
    display_name: "DeepSeek R1T Chimera"
    category: large
    capabilities:
      - reasoning
      - coding
      - hybrid
      - efficient
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 30
      tokens_per_minute: 50000
    estimated_cost:
      input_per_1k: 0.0
      output_per_1k: 0.0

  # ==================== Meta Models ====================
  llama-3.1-70b:
    provider: meta
    model_id: "meta-llama/llama-3.1-70b-instruct"
    display_name: "Llama 3.1 70B"
    category: large
    capabilities:
      - general
      - open-source
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 40
      tokens_per_minute: 60000
    estimated_cost:
      input_per_1k: 0.0007
      output_per_1k: 0.0007

  llama-3.1-8b:
    provider: meta
    model_id: "meta-llama/llama-3.1-8b-instruct"
    display_name: "Llama 3.1 8B"
    category: small
    capabilities:
      - general
      - open-source
      - fast
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 100
      tokens_per_minute: 100000
    estimated_cost:
      input_per_1k: 0.00006
      output_per_1k: 0.00006

  # ==================== Mistral Models ====================
  mistral-large:
    provider: mistral
    model_id: "mistralai/mistral-large"
    display_name: "Mistral Large"
    category: large
    capabilities:
      - general
      - coding
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 50
      tokens_per_minute: 80000
    estimated_cost:
      input_per_1k: 0.002
      output_per_1k: 0.006

  mistral-small-3.1:
    provider: mistral
    model_id: "mistralai/mistral-small-3.1"
    display_name: "Mistral Small 3.1"
    category: small
    capabilities:
      - general
      - multimodal
      - fast
    parameters:
      temperature: 0.7
      max_tokens: 4000
    rate_limit:
      requests_per_minute: 100
      tokens_per_minute: 100000
    estimated_cost:
      input_per_1k: 0.0002
      output_per_1k: 0.0006