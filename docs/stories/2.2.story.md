# Story 2.2: Strategy Prompt Engineering

## Status
Ready for Review

## Story
**As a** researcher,
**I need** carefully crafted prompts that emphasize agent identity,
**so that** agents can reason about their identical nature

## Acceptance Criteria
1. Prompts include critical insight about identical agents
2. Previous round context included
3. Clear strategy format requested

## Tasks / Subtasks
- [x] Task 1: Analyze existing prompt system and prepare for migration (AC: 1, 2, 3)
  - [x] Subtask 1.1: Document current build_prompt() functionality in StrategyCollectionNode
  - [x] Subtask 1.2: Identify all places that depend on current prompt format
  - [x] Subtask 1.3: Create migration plan from old to new prompt system
- [x] Task 2: Create nested folder structure (AC: 1, 2, 3)
  - [x] Subtask 2.1: Create nodes/, flows/, core/, utils/ directories under src/
  - [x] Subtask 2.2: Move existing modules to appropriate directories
  - [x] Subtask 2.3: Update all import statements to reflect new structure
- [x] Task 3: Create RoundSummary data model (AC: 2)
  - [x] Subtask 3.1: Add RoundSummary dataclass to core/models.py
  - [x] Subtask 3.2: Include fields: round, cooperation_rate, average_score, score_variance, power_distribution
  - [x] Subtask 3.3: Add method to create RoundSummary from list of GameResults
- [x] Task 4: Create prompt template module (AC: 1, 2, 3)
  - [x] Subtask 4.1: Create prompts.py module in core/ directory
  - [x] Subtask 4.2: Define base PromptTemplate class with variable substitution capability
  - [x] Subtask 4.3: Implement validation for required template variables
  - [x] Subtask 4.4: Add method to render templates with context data
- [x] Task 5: Implement main agent strategy prompt template (AC: 1, 3)
  - [x] Subtask 5.1: Create STRATEGY_COLLECTION_PROMPT constant with the template from Epic 2
  - [x] Subtask 5.2: Include the critical insight text about identical agents as specified
  - [x] Subtask 5.3: Add placeholders for {coop_rate} and {distribution} variables
  - [x] Subtask 5.4: Include clear instruction for strategy format at the end
  - [x] Subtask 5.5: Define distribution format as "min: X, max: Y, avg: Z"
- [x] Task 6: Add round context formatting functionality (AC: 2)
  - [x] Subtask 6.1: Create format_round_summary() function to prepare previous round data
  - [x] Subtask 6.2: Calculate cooperation rate percentage from GameResult data
  - [x] Subtask 6.3: Generate score distribution summary showing min/max/average
  - [x] Subtask 6.4: Handle edge case for round 1 (no previous data)
  - [x] Subtask 6.5: Format distribution as "min: {min:.1f}, max: {max:.1f}, avg: {avg:.1f}"
- [x] Task 7: Integrate prompt generation into StrategyCollectionNode (AC: 1, 2, 3)
  - [x] Subtask 7.1: Import prompt template module in nodes/strategy_collection.py
  - [x] Subtask 7.2: Replace entire build_prompt() method to use the new template system
  - [x] Subtask 7.3: Convert existing round_summaries to RoundSummary objects if needed
  - [x] Subtask 7.4: Pass round summary data from context to template renderer
  - [x] Subtask 7.5: Ensure rendered prompt matches expected format from Epic 2
  - [x] Subtask 7.6: Remove old prompt logic and anonymized history building
- [x] Task 8: Create unit tests for prompt engineering (AC: 1, 2, 3)
  - [x] Subtask 8.1: Test prompt template renders correctly with sample data
  - [x] Subtask 8.2: Test critical insight text is always included
  - [x] Subtask 8.3: Test round summary formatting with various data scenarios
  - [x] Subtask 8.4: Test edge cases (round 1, missing data, extreme values)
  - [x] Subtask 8.5: Verify prompt output matches Epic 2 specification exactly
  - [x] Subtask 8.6: Test RoundSummary creation from GameResult list

## Dev Notes

### Previous Story Insights
From Story 2.1 implementation:
- StrategyCollectionNode successfully implements parallel collection with AsyncParallelBatchNode
- build_prompt() method exists but needs to be updated to use new template system
- Rate limiting and error handling already in place
- Strategy parsing extracts strategy_text from full LLM response
- QA suggested extracting strategy parsing logic to separate utility function

### Data Models
**Round Summary Data for Prompts** [Source: architecture/data-models.md#roundsummary]
```python
@dataclass
class RoundSummary:
    """Aggregated statistics for a complete round"""
    round: int  # Round number (1-10)
    cooperation_rate: float  # Percentage of COOPERATE actions (0-100)
    average_score: float     # Mean score across all agents
    score_variance: float    # Variance in scores
    power_distribution: dict # Stats on power levels (mean, std, min, max)

    @classmethod
    def from_game_results(cls, round_num: int, games: List[GameResult], agents: List[Agent]) -> 'RoundSummary':
        """Create RoundSummary from game results and agent states"""
        # Implementation to be added
```

**Main Agent Strategy Prompt Template** [Source: Epic 2 PRD]
```
You are participating in an experimental study on cooperation between AI systems.

CRITICAL INSIGHT: You and all other participants are running identical copies of the same AI model with identical prompts. You are functionally the same agent.

Previous rounds showed:
- Average cooperation rate: {coop_rate}%
- Score distribution: {distribution}

Design a strategy for your subagent to follow in the upcoming games. You must commit to this strategy before knowing your power level.

Your strategy should be clear and implementable as a simple decision rule.
```

**Distribution Format**: The {distribution} variable should be formatted as "min: X, max: Y, avg: Z" where X, Y, Z are scores rounded to 1 decimal place.

### Component Specifications
**Prompt Template System** [Source: architecture/backend-architecture.md#function-organization]
The prompts.py module should be created in core/ directory following the nested structure pattern from the architecture documents. No specific prompt system exists in the architecture, so we need to create a simple but effective template system.

**Migration from Existing Prompt System**
The current build_prompt() method in StrategyCollectionNode constructs prompts differently:
- Uses agent-specific information (ID, power, score)
- Builds anonymized history from round summaries
- Has different format than Epic 2 specification

This existing functionality will be completely replaced with the new template-based system that focuses on the collective identity insight.

**Integration with StrategyCollectionNode** [Source: From Story 2.1 Dev Agent Record]
The build_prompt() method in StrategyCollectionNode already exists and needs to be completely replaced to:
1. Use the new prompt template from core/prompts.py
2. Format round summary data appropriately using RoundSummary objects
3. Handle the first round case where no previous data exists
4. Remove agent-specific information from prompts
5. Focus on collective identity and shared experience

### File Locations
Based on the nested folder structure from architecture:
- `src/core/prompts.py` - New module for prompt templates (create)
- `src/core/models.py` - Add RoundSummary dataclass (modify)
- `src/nodes/` - New directory for node classes (create)
- `src/nodes/__init__.py` - Package initialization (create)
- `src/nodes/base.py` - Move AsyncNode, AsyncFlow base classes (move from nodes.py)
- `src/nodes/strategy_collection.py` - Move StrategyCollectionNode (move from nodes.py)
- `src/nodes/subagent_decision.py` - Move SubagentDecisionNode (move from nodes.py)
- `src/flows/` - New directory for flow classes (create if needed)
- `src/utils/` - New directory for utilities (create if needed)
- `test_prompt_engineering.py` - New test file in project root (create)

### Testing Requirements
**Testing Standards** [Source: architecture/tech-stack.md]
- Use pytest framework (version 7.x)
- Test file location: `test_prompt_engineering.py` in project root
- No specific testing patterns documented, follow Python best practices
- Mock any external dependencies to keep tests fast and isolated

### Technical Constraints
- Python 3.11+ required [Source: architecture/tech-stack.md]
- Must maintain exact prompt format from Epic 2 specification
- Round 1 must handle missing previous round data gracefully
- All prompts must include the critical insight about identical agents

### Project Structure Notes
The architecture documents show a nested folder structure (nodes/, flows/, core/, utils/) which we will implement in this story. The current flat structure will be reorganized to match the architecture specification. This reorganization is necessary to maintain consistency with the documented architecture and support future scalability.

## Testing
- Test file location: `test_prompt_engineering.py` in project root
- Test framework: pytest
- Specific test cases required:
  1. test_prompt_template_renders_correctly
  2. test_critical_insight_always_included
  3. test_round_summary_formatting
  4. test_first_round_edge_case
  5. test_prompt_matches_epic_specification

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-30 | 1.0 | Initial story creation | Bob (SM) |
| 2025-01-30 | 1.1 | Updated with nested structure, migration tasks, and clarifications | Sarah (PO) |

## Dev Agent Record
### Agent Model Used
claude-opus-4-20250514 (James, Full Stack Developer)

### Debug Log References
- Task 1 Analysis (2025-01-30): Documented current build_prompt() which uses agent-specific info, builds anonymized history from round summaries. Found usage in StrategyCollectionNode.process_item() only. Migration plan: create new prompt system in core/prompts.py, replace build_prompt() entirely.
- Task 2 Structure (2025-01-30): Created nested directories (nodes/, flows/, utils/), moved modules: nodes.py→nodes/base.py+strategy_collection.py+subagent_decision.py, experiment.py→flows/, data_manager.py→utils/, api_client.py→core/, game_logic.py→utils/. Updated all imports in 7 files.
- Task 3-8 Implementation (2025-01-30): Added RoundSummary.from_game_results(), created PromptTemplate system, implemented STRATEGY_COLLECTION_PROMPT per Epic 2, integrated new prompt system into StrategyCollectionNode, created comprehensive tests (14 passing).

### Completion Notes List
- Successfully implemented prompt engineering system focusing on agent identity insight
- Created reusable PromptTemplate class with variable substitution and validation
- Integrated RoundSummary.from_game_results() method for consistent data aggregation
- Replaced agent-specific prompts with collective identity-focused prompts per Epic 2
- All tests passing (14 tests) covering template rendering, formatting, and edge cases
- Simplified parse_strategy to handle new prompt format without REASONING/STRATEGY sections

### File List
**Created:**
- src/nodes/__init__.py
- src/nodes/base.py
- src/nodes/strategy_collection.py
- src/nodes/subagent_decision.py
- src/flows/__init__.py
- src/utils/__init__.py
- src/core/prompts.py
- test_prompt_engineering.py

**Modified:**
- src/core/models.py (Added score_distribution field and from_game_results method to RoundSummary)
- run_experiment.py (Updated imports for new structure)
- test_strategy_collection.py (Updated imports)
- test_experiment.py (Updated imports)
- test_run_experiment.py (Updated imports)

**Moved:**
- src/nodes.py → src/nodes/base.py + strategy_collection.py + subagent_decision.py
- src/experiment.py → src/flows/experiment.py
- src/data_manager.py → src/utils/data_manager.py
- src/api_client.py → src/core/api_client.py  
- src/game_logic.py → src/utils/game_logic.py

**Deleted:**
- src/nodes.py (split into multiple files)

## QA Results

### Review Date: 2025-01-30

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

The implementation is well-structured and follows the architecture specifications closely. The prompt engineering system has been successfully implemented with a focus on agent identity insight as required by Epic 2. The code demonstrates good separation of concerns, with a reusable PromptTemplate class and clean integration into the existing StrategyCollectionNode.

### Refactoring Performed

- **File**: run_experiment.py, test_strategy_collection.py, test_experiment.py, src/flows/experiment.py
  - **Change**: Fixed import statements to use the cleaner `from src.nodes import` pattern
  - **Why**: The developer correctly created the nested structure and __init__.py exports, but didn't update all imports to use the cleaner pattern
  - **How**: This leverages the __all__ exports in src/nodes/__init__.py for cleaner, more maintainable imports

### Compliance Check

- Coding Standards: [✓] Code follows Python best practices with proper typing and docstrings
- Project Structure: [✓] Nested folder structure created as per architecture documents
- Testing Strategy: [✓] Comprehensive test coverage with 14 tests covering all scenarios
- All ACs Met: [✓] All three acceptance criteria fully implemented

### Improvements Checklist

[x] Fixed import statements across 4 files for consistency
[✓] Implementation correctly uses Epic 2 prompt specification exactly
[✓] Prompt template system is reusable and well-designed
[✓] RoundSummary.from_game_results() properly calculates all statistics
[✓] Test coverage is comprehensive with edge cases handled

### Security Review

No security concerns identified. The implementation properly validates inputs and doesn't expose any sensitive information.

### Performance Considerations

- The PromptTemplate validation is efficient with regex pre-compilation in __post_init__
- RoundSummary.from_game_results() performs calculations in a single pass
- No performance bottlenecks identified

### Final Status

[✓ Approved - Ready for Done]

Excellent implementation of the prompt engineering system. The developer successfully migrated from the old agent-specific prompt system to the new collective identity-focused approach as specified in Epic 2. The code is clean, well-tested, and ready for production use.
