# Story 6.4: Mixed Model Scenario Testing

## Status
Done

## Story
**As a** researcher,
**I need** to test specific mixed-model ratios (e.g., 5 GPT-4 + 5 Claude),
**So that** I can study how model diversity affects overall cooperation

## Acceptance Criteria
1. Experiments support predefined model ratios
2. Random assignment maintains specified ratios
3. Results segmented by model composition
4. Tracks emergence of model-specific coalitions
5. Measures impact of model diversity on cooperation rates

## Tasks / Subtasks
- [x] Task 1: Create scenario execution framework (AC: 1, 2)
  - [x] Subtask 1.1: Create new file `src/core/scenario_manager.py` with ScenarioManager class
  - [x] Subtask 1.2: Implement validate_scenario() method to ensure model counts sum to NUM_AGENTS
  - [x] Subtask 1.3: Add assign_models_to_agents() method that maintains exact ratios with random distribution
  - [x] Subtask 1.4: Create scenario execution tracking to store which agents have which models
  - [x] Subtask 1.5: Add scenario persistence to save model assignments per experiment
  - [x] Subtask 1.6: Write unit tests for ratio validation and assignment algorithms
- [x] Task 2: Enhance experiment runner for scenario support (AC: 1, 3)
  - [x] Subtask 2.1: Update run_experiment.py to accept --scenario parameter
  - [x] Subtask 2.2: Load scenario configuration from YAML files in configs/examples/multi_model/
  - [x] Subtask 2.3: Integrate ScenarioManager into ExperimentFlow initialization
  - [x] Subtask 2.4: Add scenario name and model distribution to experiment metadata
  - [x] Subtask 2.5: Create validation for scenario consistency across rounds
  - [x] Subtask 2.6: Write integration tests for scenario-based experiment execution
- [x] Task 3: Implement model diversity metrics (AC: 5)
  - [x] Subtask 3.1: Add calculate_model_diversity() method using Shannon entropy
  - [x] Subtask 3.2: Create diversity impact analyzer comparing cooperation rates vs diversity score
  - [x] Subtask 3.3: Track diversity evolution if models can be eliminated/added
  - [x] Subtask 3.4: Add statistical tests for diversity-cooperation correlation
  - [x] Subtask 3.5: Generate diversity impact visualizations
  - [x] Subtask 3.6: Write tests for diversity calculations with edge cases
- [x] Task 4: Extend coalition detection for mixed scenarios (AC: 4)
  - [x] Subtask 4.1: Enhance CrossModelAnalyzer.analyze_model_coalitions() for mixed scenarios
  - [x] Subtask 4.2: Add temporal coalition tracking across rounds
  - [x] Subtask 4.3: Implement coalition stability metrics (how long coalitions last)
  - [x] Subtask 4.4: Detect cross-model vs same-model coalition formation rates
  - [x] Subtask 4.5: Add coalition visualization for mixed model scenarios
  - [x] Subtask 4.6: Write tests for coalition detection in various model distributions
- [x] Task 5: Create results segmentation by scenario (AC: 3)
  - [x] Subtask 5.1: Update DataManager to create scenario-specific subdirectories
  - [x] Subtask 5.2: Modify save methods to include scenario context in filenames
  - [x] Subtask 5.3: Create ScenarioComparator utility to analyze across scenarios
  - [x] Subtask 5.4: Add scenario filtering to AnalysisNode reporting
  - [x] Subtask 5.5: Generate comparative reports across different model ratios
  - [x] Subtask 5.6: Write tests for scenario-based data organization
- [x] Task 6: Implement predefined scenario templates (AC: 1, 2)
  - [x] Subtask 6.1: Create scenario template loader supporting various formats
  - [x] Subtask 6.2: Implement "balanced" scenarios (equal distribution)
  - [x] Subtask 6.3: Implement "majority-minority" scenarios (7-3, 8-2, 9-1 splits)
  - [x] Subtask 6.4: Implement "diverse" scenarios (3+ different models)
  - [x] Subtask 6.5: Add scenario validation ensuring consistency
  - [x] Subtask 6.6: Create scenario generation CLI tool for custom ratios
- [x] Task 7: Add scenario-specific analysis features (AC: 3, 4, 5)
  - [x] Subtask 7.1: Create inter-model cooperation heatmaps per scenario
  - [x] Subtask 7.2: Add minority model performance tracking
  - [x] Subtask 7.3: Implement "model dominance" detection (which model drives cooperation)
  - [x] Subtask 7.4: Track strategy convergence/divergence by model composition
  - [x] Subtask 7.5: Generate scenario comparison dashboards
  - [x] Subtask 7.6: Write comprehensive scenario analysis tests

## Dev Notes

### Previous Story Insights
Story 6.3 successfully implemented cross-model cooperation analysis with:
- CrossModelAnalyzer utility class for analyzing cooperation patterns between models
- Enhanced AnalysisNode with model-specific pattern detection
- Coalition detection capabilities in analyze_model_coalitions() method
- Cooperation matrix calculation and in-group bias detection
- Integration into the main analysis workflow
[Source: docs/stories/6.3.story.md#Dev-Agent-Record]

### Data Models
ScenarioConfig already exists from Story 6.1:
```python
@dataclass
class ScenarioConfig:
    name: str  # e.g., "homogeneous_gpt4", "mixed_5_5", "diverse_3_3_4"
    model_distribution: Dict[str, int]  # {"gpt-4": 5, "claude-3": 5}
```
[Source: docs/stories/6.1.story.md#L24]

ExperimentFlow already accepts scenario configuration:
```python
# From Story 6.1 implementation
if config.ENABLE_MULTI_MODEL and scenario:
    self._assign_models_to_agents(agents, scenario)
```
[Source: docs/stories/6.1.story.md#L47-49]

### Scenario Templates from Epic
From Epic 6, the following scenarios should be implemented:
1. **Homogeneous Scenarios** (Baseline)
   - 10 GPT-4 agents only
   - 10 Claude-3 agents only
   - 10 Gemini-Pro agents only

2. **Balanced Mixed Scenarios**
   - 5 GPT-4 + 5 Claude-3
   - 5 GPT-4 + 5 Gemini-Pro
   - 5 Claude-3 + 5 Gemini-Pro

3. **Diverse Scenarios**
   - 3 GPT-4 + 3 Claude-3 + 4 Gemini-Pro
   - 2 of each of 5 different models

4. **Asymmetric Scenarios**
   - 7 GPT-4 + 3 Claude-3 (majority-minority)
   - 1 GPT-4 + 9 Claude-3 (singleton)
[Source: docs/prd/epic-6-multi-model-experiments.md#L227-245]

### File Locations
Based on project structure:
- `src/core/scenario_manager.py` - NEW file for scenario management
- `src/utils/scenario_comparator.py` - NEW file for cross-scenario analysis
- `test_scenario_manager.py` - NEW test file in project root
- Scenario results stored in: `results/{experiment_id}/scenarios/{scenario_name}/`
[Source: Project structure patterns]

### Integration Points
1. **run_experiment.py**: Add --scenario CLI parameter
2. **ExperimentFlow**: Already has scenario support from Story 6.1
3. **CrossModelAnalyzer**: Already tracks coalitions from Story 6.3
4. **DataManager**: Needs extension for scenario-based organization
5. **AnalysisNode**: Already segments by model from Story 6.2

### Technical Constraints
- Must maintain NUM_AGENTS total (default 10) across all model distributions
- Random assignment must be deterministic (use fixed seed per experiment)
- Scenario names must be filesystem-safe for directory creation
- Model diversity calculations should handle edge cases (single model = 0 diversity)
[Source: docs/architecture/backend-architecture.md#Database-Architecture]

### Diversity Metrics
Shannon Entropy for model diversity:
```python
H = -Σ(p_i * log(p_i))  # where p_i is proportion of model i
# Examples:
# 10 same model: H = 0 (no diversity)
# 5+5 split: H = 0.693 (moderate diversity)  
# 3+3+4 split: H = 1.08 (high diversity)
```

### Performance Considerations
- Scenario execution should not add significant overhead
- Model assignments cached per experiment to avoid recalculation
- Batch scenario comparisons for efficient analysis
- Memory-efficient storage of scenario-specific results
[Source: docs/prd/epic-6-multi-model-experiments.md#L267-271]

## QA Results

### Review Date: 2025-08-02

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

The implementation is comprehensive and well-structured, demonstrating solid software engineering practices. The developer has successfully created a robust scenario management system that enables mixed-model experiments with proper tracking and analysis capabilities. The code follows good separation of concerns with dedicated components for scenario management, diversity analysis, coalition tracking, and cross-scenario comparison.

### Refactoring Performed

- **File**: src/core/scenario_manager.py
  - **Change**: Removed duplicate ScenarioConfig definition and imported from models.py instead
  - **Why**: ScenarioConfig was defined in both scenario_manager.py and models.py, causing confusion
  - **How**: This eliminates the duplication and ensures single source of truth for the data model

### Compliance Check

- Coding Standards: [✓] Well-documented code with proper docstrings and type hints
- Project Structure: [✓] Files properly organized in src/core, src/utils as specified
- Testing Strategy: [✓] Comprehensive test coverage with unit and integration tests
- All ACs Met: [✓] All 5 acceptance criteria fully implemented

### Improvements Checklist

[x] Removed duplicate ScenarioConfig definition from scenario_manager.py
[ ] Add validation for model names against available models in config
[ ] Consider adding scenario template validation against NUM_AGENTS at load time
[ ] Add more detailed logging for scenario assignment process
[ ] Consider adding scenario comparison visualization exports

### Security Review

No security concerns identified. The code properly validates inputs, uses safe file operations, and doesn't expose sensitive data.

### Performance Considerations

- Scenario assignments are efficiently cached per experiment
- Diversity calculations use numpy for performance
- Coalition tracking maintains efficient data structures
- File I/O is minimized with batch operations

### Architecture Notes

The implementation shows good architectural decisions:
1. ScenarioManager as central authority for model assignments
2. Proper separation between analysis components (diversity, coalitions, comparison)
3. Good use of dataclasses for type safety
4. Temporal tracking integrated cleanly with existing CrossModelAnalyzer

One architectural concern: ScenarioConfig is defined in both scenario_manager.py and models.py, which could lead to confusion. Recommend consolidating to one location.

### Test Coverage Assessment

Excellent test coverage across all components:
- Unit tests for core functionality with edge cases
- Integration tests for scenario loading and execution
- Statistical tests for diversity and coalition analysis
- Proper mocking for API-dependent components

### Final Status

[✓ Approved - Ready for Done]

The implementation successfully addresses all acceptance criteria with high-quality, well-tested code. The minor import issue has been fixed, and the suggested improvements are non-blocking enhancements for future iterations.

## Testing
- Test file location: `test_scenario_manager.py` in project root
- Integration tests: Add to existing test_experiment.py
- Test framework: pytest with pytest-asyncio
- Mock scenario configurations for testing
- Test coverage requirements:
  - Scenario validation and model assignment
  - Ratio maintenance across multiple assignments
  - Diversity calculations with various distributions
  - Coalition detection in mixed scenarios
  - Scenario-based data organization
  - Integration with existing multi-model infrastructure
  - Edge cases: single model, many models, odd distributions

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
_To be filled by development agent_

### Agent Model Used
claude-opus-4-20250514

### Debug Log References
_To be filled by dev agent_

### Completion Notes List
1. Implemented comprehensive ScenarioManager with model assignment and diversity calculations
2. Enhanced run_experiment.py to accept --scenario parameter for CLI usage
3. Created YAML scenario templates covering all required patterns (homogeneous, balanced, diverse, asymmetric)
4. Integrated ScenarioManager with ExperimentFlow for seamless scenario execution
5. Built DiversityAnalyzer with Shannon entropy calculations and statistical correlation analysis
6. Extended CrossModelAnalyzer with temporal coalition tracking for mixed scenarios
7. Updated DataManager to organize results by scenario with proper directory structure (src/core as requested)
8. Created ScenarioComparator for cross-scenario analysis and reporting
9. Built scenario_tool.py CLI for scenario generation and management
10. Implemented comprehensive visualization utilities for diversity impact and coalition networks
11. Added ScenarioAnalyzer for detailed mixed-model analysis (minority tracking, dominance detection)
12. All tests passing with good coverage of edge cases and integration points

### File List
- src/core/scenario_manager.py (NEW)
- test_scenario_manager.py (NEW)
- run_experiment.py (MODIFIED)
- src/utils/scenario_loader.py (NEW)
- configs/examples/multi_model/balanced_scenarios.yaml (NEW)
- configs/examples/multi_model/diverse_scenarios.yaml (NEW)
- configs/examples/multi_model/asymmetric_scenarios.yaml (NEW)
- configs/examples/multi_model/homogeneous_scenarios.yaml (NEW)
- test_scenario_integration.py (NEW)
- src/utils/diversity_analyzer.py (NEW)
- src/utils/diversity_statistics.py (NEW)
- src/utils/diversity_visualizer.py (NEW)
- test_diversity_analysis.py (NEW)
- src/utils/coalition_tracker.py (NEW)
- src/utils/cross_model_analyzer.py (MODIFIED)
- src/utils/coalition_visualizer.py (NEW)
- test_coalition_tracking.py (NEW)
- src/utils/data_manager.py (MODIFIED)
- src/utils/scenario_comparator.py (NEW)
- scenario_tool.py (NEW)
- test_scenario_organization.py (NEW)
- src/utils/scenario_analyzer.py (NEW)
- test_scenario_analyzer.py (NEW)

## Next Developer Context

This story builds on the multi-model infrastructure from Stories 6.1-6.3. The key integration points are:

1. **ScenarioConfig** and model assignment logic already exist from Story 6.1
2. **CrossModelAnalyzer** from Story 6.3 provides coalition detection capabilities
3. **Model-specific tracking** from Story 6.2 enables proper segmentation

The main focus is creating a robust scenario execution framework that:
- Manages predefined model distributions
- Ensures consistent ratios across experiments
- Provides scenario-specific analysis and reporting
- Enables comparison across different model compositions

Critical implementation notes:
1. ScenarioManager should be the single source of truth for model assignments
2. Model assignments must be persisted for experiment reproducibility
3. Diversity metrics should be calculated at initialization and after each round
4. Coalition detection needs temporal tracking to identify emergence patterns
5. Results must be organized by scenario for easy comparison