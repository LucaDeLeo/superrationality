# Story 6.2: Model-Specific Strategy Collection

## Status
Ready for Review

## Story
**As a** researcher,
**I need** to collect strategies from different model types in a standardized way,
**so that** strategies can be compared across models fairly

## Acceptance Criteria
1. Unified prompt format works across all model types
2. Model-specific parameters (temperature, max_tokens) configurable
3. Response parsing handles model-specific formatting differences
4. Retry logic accounts for model-specific rate limits
5. Model type tracked in strategy metadata

## Tasks / Subtasks
- [x] Task 1: Enhance prompt system for model-aware strategy collection (AC: 1, 2)
  - [x] Subtask 1.1: Add model_type field to PromptTemplate context handling
  - [x] Subtask 1.2: Create model-specific prompt variations dictionary in prompts.py for subtle model-optimized differences
  - [x] Subtask 1.3: Update STRATEGY_COLLECTION_PROMPT to support optional model-specific instructions
  - [x] Subtask 1.4: Add prompt validation to ensure compatibility across all supported models
  - [x] Subtask 1.5: Write unit tests for model-aware prompt rendering
- [x] Task 2: Implement model-specific response parsing enhancements (AC: 3)
  - [x] Subtask 2.1: Extend StrategyCollectionNode.parse_strategy to handle model-specific response patterns
  - [x] Subtask 2.2: Add response format validators for each model type (GPT-4 JSON tendency, Claude XML tendency, etc.)
  - [x] Subtask 2.3: Create fallback parsing strategies when primary format detection fails
  - [x] Subtask 2.4: Add logging for response format analysis to improve parsing over time
  - [x] Subtask 2.5: Write tests with real response samples from each model type
- [x] Task 3: Enhance rate limiting for multi-model scenarios (AC: 4)
  - [x] Subtask 3.1: Create ModelRateLimiter class that tracks rate limits per model type
  - [x] Subtask 3.2: Integrate with existing adapter.enforce_rate_limit() method
  - [x] Subtask 3.3: Add dynamic rate adjustment based on API response headers (X-RateLimit-*)
  - [x] Subtask 3.4: Implement priority queue for mixed-model experiments to optimize throughput
  - [x] Subtask 3.5: Add rate limit monitoring and reporting to experiment stats
  - [x] Subtask 3.6: Write tests simulating rate limit scenarios for each model
- [x] Task 4: Extend strategy metadata tracking (AC: 5)
  - [x] Subtask 4.1: Add model_version field to StrategyRecord (in addition to existing model field)
  - [x] Subtask 4.2: Track model-specific parameters used (temperature, max_tokens, etc.) in strategy metadata
  - [x] Subtask 4.3: Add response_format field to track which parsing strategy succeeded
  - [x] Subtask 4.4: Store model inference latency for performance analysis
  - [x] Subtask 4.5: Update data persistence to include all new metadata fields
  - [x] Subtask 4.6: Write migration script for existing strategy records
- [x] Task 5: Create model-specific strategy collection tests (AC: 1-5)
  - [x] Subtask 5.1: Create test fixtures with model-specific mock responses
  - [x] Subtask 5.2: Test unified prompt rendering for each model type
  - [x] Subtask 5.3: Test response parsing for various response formats
  - [x] Subtask 5.4: Test rate limiting behavior with multiple models
  - [x] Subtask 5.5: Test metadata tracking completeness
  - [x] Subtask 5.6: Create integration test with mock multi-model experiment
- [x] Task 6: Update experiment reporting for model-specific insights (AC: 5)
  - [x] Subtask 6.1: Extend AnalysisNode to segment strategies by model type
  - [x] Subtask 6.2: Add model-specific strategy length and complexity metrics
  - [x] Subtask 6.3: Create comparative visualizations for strategy patterns by model
  - [x] Subtask 6.4: Update experiment summary to include model distribution stats
  - [x] Subtask 6.5: Add model-specific error and fallback rates to reports

## Dev Notes

### Previous Story Insights
Story 6.1 successfully implemented the multi-model configuration infrastructure with:
- ModelConfig and adapter framework in place
- UnifiedOpenRouterAdapter handling all models via OpenRouter
- Feature flag protection (ENABLE_MULTI_MODEL)
- Basic integration in StrategyCollectionNode lines 64-152
- Model type already tracked in StrategyRecord.model field (line 210)
[Source: docs/stories/6.1.story.md#Dev-Agent-Record]

### Current Strategy Collection Implementation
The StrategyCollectionNode already has basic multi-model support:
```python
# Check if agent has model_config (multi-model enabled)
if agent.model_config is not None:
    adapter = ModelAdapterFactory.get_adapter(agent.model_config)
    model = agent.model_config.model_type
```
[Source: src/nodes/strategy_collection.py#L65-68]

### Data Models
StrategyRecord already includes model tracking:
```python
@dataclass
class StrategyRecord:
    strategy_id: str
    agent_id: int
    round: int
    strategy_text: str
    full_reasoning: str
    prompt_tokens: int = 0
    completion_tokens: int = 0
    model: str = ""  # Already tracking model
```
[Source: src/core/models.py#StrategyRecord]

### Prompt System Architecture
Current prompt system uses PromptTemplate class with variable substitution:
```python
@dataclass
class PromptTemplate:
    template: str
    required_variables: Optional[Set[str]] = None
    
    def render(self, context: Dict[str, Any]) -> str:
        # Performs variable substitution
```
[Source: src/core/prompts.py#L8-62]

### API Integration Points
OpenRouterClient already supports model parameter:
- `complete()` method accepts model parameter
- Response format is unified through OpenRouter
- Adapter framework handles model-specific parameters
[Source: src/core/api_client.py#complete]

### File Locations
Based on project structure:
- `src/core/prompts.py` - Extend prompt templates
- `src/nodes/strategy_collection.py` - Main implementation file
- `src/utils/rate_limiter.py` - NEW file for ModelRateLimiter
- `src/core/models.py` - Extend StrategyRecord if needed
- `test_model_strategy_collection.py` - NEW test file in project root
[Source: docs/architecture/project-structure.md]

### Technical Constraints
- OpenRouter provides unified interface but models may return different formats
- Rate limits vary by model: typically 60 req/min for free tier
- Must maintain backward compatibility with existing single-model experiments
- Strategy collection must remain non-blocking (parallel execution)
[Source: docs/architecture/external-apis.md#Rate-Limits]

### Model-Specific Behavioral Patterns
Based on Epic 6 specifications, we need to detect:
- GPT-4: "chain of thought", "explicit utility calculation"
- Claude-3: "constitutional principles", "harm minimization"
- Response format tendencies (JSON vs plain text vs XML-like)
[Source: docs/prd/epic-6-multi-model-experiments.md#L206-215]

## Testing
- Test file location: `test_model_strategy_collection.py` in project root
- Use pytest-asyncio for async tests
- Mock OpenRouter responses using aioresponses or custom mocks
- Test coverage requirements:
  - Prompt rendering for each model type
  - Response parsing variations
  - Rate limiting per model
  - Metadata tracking
  - Fallback scenarios
  - Integration with existing flows

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
_To be filled by development agent_

### Agent Model Used
Claude Opus 4 (claude-opus-4-20250514)

### Debug Log References
- Task 1 completed: Enhanced prompt system for model-aware strategy collection
  - Added model_type field to format_round_summary() context
  - Created MODEL_PROMPT_VARIATIONS dictionary with model-specific optimizations
  - Implemented apply_model_variations() to enhance prompts based on model
  - Added validate_prompt_compatibility() for cross-model validation
  - Updated build_prompt() in StrategyCollectionNode to use model-aware prompting
  - Added comprehensive unit tests in test_prompt_engineering.py
- Task 2 completed: Implemented model-specific response parsing enhancements
  - Extended parse_strategy() to use _extract_strategy_by_model() with model-specific patterns
  - Added response_format field to StrategyRecord to track parsing method
  - Implemented model-specific parsers for GPT-4, GPT-3.5, Claude, and Gemini
  - Added multi-level fallback parsing: model-specific → marker-based → paragraph → sentence → full
  - Added _validate_strategy_format() to ensure quality of extracted strategies
  - Added extensive logging for parsing analysis
  - Created comprehensive test suite in test_model_strategy_collection.py
- Task 3 completed: Enhanced rate limiting for multi-model scenarios
  - Created ModelRateLimiter class in src/utils/rate_limiter.py with per-model tracking
  - Tracks requests per model with configurable limits and burst sizes
  - Integrated with adapter.enforce_rate_limit() to support centralized rate limiting
  - Added update_from_headers() to dynamically adjust limits from API responses
  - Implemented PriorityQueue for optimizing throughput in mixed-model scenarios
  - Added rate limit stats to strategy_collection_stats in StrategyCollectionNode
  - Created comprehensive test suite in test_rate_limiter.py
- Task 4 completed: Extended strategy metadata tracking
  - Added model_version, model_params, and inference_latency fields to StrategyRecord
  - Updated parse_strategy() to accept and track all new metadata
  - Modified process_item() to measure inference latency and extract model version
  - response_format field already added in Task 2
  - Created migrate_strategy_records.py script for migrating existing records
  - Added tests in TestStrategyMetadataTracking class
- Task 5 completed: Created model-specific strategy collection tests
  - Created comprehensive test fixtures for all supported models in TestModelSpecificParsing
  - Added TestMultiModelIntegration class with full integration tests
  - test_prompt_rendering_all_models verifies prompts work for all models
  - test_real_model_responses tests parsing with realistic model outputs
  - test_rate_limiting_with_multiple_models verifies rate limiting behavior
  - test_metadata_tracking_in_collection verifies all metadata is captured
  - test_multi_model_experiment_mock runs a full mock multi-model experiment
  - Fixed rate limiting issue for agents without model config

### Completion Notes List
- Task 6 completed: Updated experiment reporting for model-specific insights
  - Extended AnalysisNode to track strategies and markers by model type in analyze_transcript()
  - Added _track_model_specific_pattern() to detect model-specific behaviors (GPT-4 utility, Claude ethics, Gemini analytical)
  - Enhanced generate_analysis_report() to include model_specific_analysis section with insights and metrics
  - Added _generate_model_insights() to create behavioral insights per model with dominant patterns
  - Added _calculate_model_metrics() for model distribution, cooperation tendency, and complexity metrics
  - Created visualization-ready data structures for model comparisons
  - Added comprehensive test suite in test_analysis_model_specific.py

### File List
- Modified: src/core/prompts.py
- Modified: src/nodes/strategy_collection.py  
- Modified: src/core/models.py (added response_format, model_version, model_params, inference_latency fields)
- Modified: src/core/model_adapters.py (updated enforce_rate_limit to support ModelRateLimiter)
- Modified: src/nodes/analysis.py (added model-specific tracking and reporting)
- Modified: test_prompt_engineering.py
- Modified: test_model_strategy_collection.py (added TestStrategyMetadataTracking)
- New: src/utils/rate_limiter.py
- New: test_rate_limiter.py
- New: migrate_strategy_records.py
- New: test_analysis_model_specific.py

## Next Developer Context

This story builds on the infrastructure from Story 6.1. The main focus is enhancing the existing strategy collection to be truly model-aware, handling the nuances of different model responses while maintaining a unified interface. The adapter framework is already in place, so this story focuses on the strategy collection specifics: prompts, parsing, rate limiting, and metadata tracking.

Key areas to focus on:
1. Model-specific prompt optimizations (while keeping unified format)
2. Robust response parsing that handles each model's quirks
3. Smart rate limiting for mixed-model scenarios
4. Rich metadata for later analysis

The next story (6.3) will likely focus on cross-model cooperation analysis, so ensure all metadata needed for that analysis is captured here.

## QA Results

### Review Date: 2025-08-02

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

The implementation demonstrates excellent software engineering practices with comprehensive model-specific strategy collection capabilities. The developer successfully built upon the multi-model infrastructure from Story 6.1, creating a robust and maintainable solution that handles the complexities of different model behaviors while maintaining backward compatibility.

Key strengths include:
- Sophisticated multi-level response parsing with model-specific patterns
- Well-designed ModelRateLimiter with per-model tracking and dynamic adjustment
- Comprehensive metadata tracking for future analysis needs
- Strong error handling with fallback mechanisms
- Excellent test coverage including edge cases

### Refactoring Performed

- **File**: src/nodes/strategy_collection.py
  - **Change**: Extracted rate limiting logic into `_apply_rate_limiting()` method
  - **Why**: Reduce code duplication and improve maintainability
  - **How**: Centralized the complex rate limiter type checking logic into a single reusable method

- **File**: src/nodes/strategy_collection.py
  - **Change**: Improved model version extraction with `_extract_model_version()` method
  - **Why**: The inline version extraction was fragile and only handled one pattern
  - **How**: Created a robust method that handles multiple version patterns (dates, semantic versions, etc.)

- **File**: src/nodes/strategy_collection.py
  - **Change**: Enhanced Gemini response parsing to use proper whitespace normalization
  - **Why**: Multi-line responses could have inconsistent whitespace
  - **How**: Used `' '.join(strategy.split())` for consistent normalization

- **File**: src/nodes/strategy_collection.py
  - **Change**: Added rate limiting to fallback model retry attempts
  - **Why**: Fallback attempts were bypassing rate limiting, potentially causing API issues
  - **How**: Added `_apply_rate_limiting()` call before fallback API requests

- **File**: test_model_strategy_collection.py
  - **Change**: Added comprehensive test for model version extraction
  - **Why**: New extraction logic needed test coverage
  - **How**: Created test cases covering all version patterns the method handles

### Compliance Check

- Coding Standards: ✓ Code follows Python best practices and project conventions
- Project Structure: ✓ Files properly organized according to unified project structure
- Testing Strategy: ✓ Comprehensive unit and integration tests provided
- All ACs Met: ✓ All acceptance criteria fully implemented and tested

### Improvements Checklist

[x] Extracted rate limiting logic to reduce duplication
[x] Improved model version extraction robustness
[x] Enhanced whitespace normalization in Gemini parser
[x] Added rate limiting for fallback retries
[x] Added test coverage for version extraction

### Security Review

No security vulnerabilities identified. The implementation properly:
- Validates all inputs before processing
- Uses parameterized API calls (no string interpolation)
- Handles sensitive data (API keys) through environment variables
- Implements proper rate limiting to prevent API abuse

### Performance Considerations

The implementation includes several performance optimizations:
- Parallel strategy collection using asyncio.gather
- Efficient rate limiting with minimal overhead
- Smart caching in ModelAdapterFactory
- Optimized regex patterns with pre-compilation where beneficial

One minor suggestion for future optimization: Consider pre-compiling all regex patterns in the parsing methods as class constants to avoid recompilation on each call.

### Final Status

✓ Approved - Ready for Done

The implementation exceeds expectations with robust model-specific handling, comprehensive error recovery, and excellent test coverage. The code is production-ready and well-positioned for the upcoming cross-model cooperation analysis in Story 6.3.