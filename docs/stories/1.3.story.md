# Story 1.3: Create Experiment Orchestration

## Status
Done

## Story
**As a** researcher,
**I need** the system to automatically run the complete 10-round experiment,
**so that** I can gather data without manual intervention

## Acceptance Criteria
1. Full experiment runs without manual intervention
2. Handles orchestration-level errors gracefully (node-level retry logic handled by Story 1.2)
3. Outputs `experiment_results.json` with complete data

## Tasks / Subtasks
- [x] Task 1: Create DataManager for file persistence (AC: 3)
  - [x] Subtask 1.1: Implement DataManager class with experiment directory creation
  - [x] Subtask 1.2: Add methods for saving strategies, games, and round summaries
  - [x] Subtask 1.3: Ensure atomic writes to prevent data corruption
  - [x] Subtask 1.4: Create directory structure (results/exp_YYYYMMDD_HHMMSS/rounds/)
- [x] Task 2: Implement experiment result tracking and saving (AC: 3)
  - [x] Subtask 2.1: Create ExperimentResult dataclass to track overall metrics
  - [x] Subtask 2.2: Implement final result compilation from all round data
  - [x] Subtask 2.3: Save experiment_results.json with complete experiment data
  - [x] Subtask 2.4: Add cost tracking for API usage
- [x] Task 3: Create main experiment runner script (AC: 1, 2)
  - [x] Subtask 3.1: Implement run_experiment.py with async main function
  - [x] Subtask 3.2: Initialize Config, DataManager, and API client instances
  - [x] Subtask 3.3: Set up proper logging configuration for console and file output
  - [x] Subtask 3.4: Implement graceful shutdown handling (Ctrl+C)
  - [x] Subtask 3.5: Implement rate limiting to respect OpenRouter API limits (60 req/min)
- [x] Task 4: Implement progress tracking and user feedback (AC: 1)
  - [x] Subtask 4.1: Add progress indicators for rounds and games
  - [x] Subtask 4.2: Display real-time cooperation rates and scores
  - [x] Subtask 4.3: Show estimated time remaining using moving average algorithm:
    - Track completion time for each round
    - Calculate moving average of last 3 rounds
    - Estimate: (10 - current_round) * avg_round_time
    - Display as "Estimated time remaining: X minutes"
  - [x] Subtask 4.4: Log all important events to console and file
- [x] Task 5: Add error recovery and partial result saving (AC: 2, 3)
  - [x] Subtask 5.1: Implement try/except blocks around main experiment loop
  - [x] Subtask 5.2: Save partial results on critical failures
  - [x] Subtask 5.3: Log all errors with full context to experiment_errors.log
  - [x] Subtask 5.4: Provide clear error messages and recovery instructions
- [x] Task 6: Write integration tests for full experiment flow (AC: 1, 2, 3)
  - [x] Subtask 6.1: Test complete experiment execution with mocked API
  - [x] Subtask 6.2: Test error handling and partial result saving
  - [x] Subtask 6.3: Verify experiment_results.json structure and content
  - [x] Subtask 6.4: Test interruption handling and graceful shutdown

## Dev Notes

### Previous Story Insights
From Story 1.2 implementation:
- Node architecture successfully implemented with AsyncNode, AsyncFlow, and AsyncParallelBatchNode base classes
- StrategyCollectionNode and SubagentDecisionNode working with proper error handling
- ExperimentFlow and RoundFlow orchestration classes created
- Retry logic with exponential backoff implemented (max 3 retries)
- Partial failure handling allows experiment to continue if single agent fails
- All errors logged to experiment_errors.log with timestamps

### Data Models
**ExperimentResult Model** [Source: architecture/data-models.md#experimentresult]
```python
# Key attributes for ExperimentResult
experiment_id: str  # Unique experiment identifier
start_time: str  # Experiment start timestamp
end_time: str  # Experiment end timestamp
total_rounds: int  # Number of rounds completed
total_games: int  # Total games played
total_api_calls: int  # API calls made
total_cost: float  # Estimated API cost
round_summaries: list  # All round summaries
acausal_indicators: dict  # Analysis results
```

**RoundSummary Model** [Source: architecture/data-models.md#roundsummary]
```python
# Key attributes for RoundSummary
round: int  # Round number (1-10)
cooperation_rate: float  # Percentage of COOPERATE actions
average_score: float  # Mean score across all agents
score_variance: float  # Variance in scores
power_distribution: dict  # Statistics on power levels
anonymized_games: list  # Games with anonymized agent IDs
strategy_similarity: float  # Cosine similarity of strategies
```

### API Specifications
**Config Class** [Source: architecture/backend-architecture.md#api-key-configuration]
```python
class Config:
    """Manages configuration and secrets"""
    def __init__(self):
        self.api_key = os.environ.get("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable required")

    @property
    def headers(self) -> dict:
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
```

### Component Specifications
**DataManager Pattern** [Source: architecture/backend-architecture.md#data-access-layer]
```python
class DataManager:
    """Handles all file I/O operations"""
    def __init__(self, base_path: str = "results"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)  # Ensure results directory exists
        self.experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.experiment_path = self.base_path / self.experiment_id

    def save_strategies(self, round_num: int, strategies: List[StrategyRecord]):
        path = self.experiment_path / "rounds" / f"strategies_r{round_num}.json"
        data = {
            "round": round_num,
            "timestamp": datetime.now().isoformat(),
            "strategies": [asdict(s) for s in strategies]
        }
        self._write_json(path, data)

    def save_games(self, round_num: int, games: List[GameResult]):
        # Similar pattern for games, summaries, etc.
```

**Context Dictionary Structure** [Source: Story 1.2 Dev Notes]
```python
context = {
    "experiment_id": str,           # Unique experiment identifier
    "round": int,                   # Current round number (1-10)
    "agents": List[Agent],          # List of all agents with current state
    "strategies": List[StrategyRecord],  # Strategies for current round
    "games": List[GameResult],      # Games played in current round
    "round_summaries": List[RoundSummary],  # Previous round summaries
    "config": Config,               # Experiment configuration
    "data_manager": DataManager,    # For saving results
}
```

### File Locations
Based on the project structure [Source: architecture/project-structure.md]:
- `run_experiment.py` - Main experiment runner script (project root)
- `src/core/config.py` - Already exists from Story 1.1
- `src/data_manager.py` - New file for DataManager class
- `src/core/models.py` - Already exists, may need ExperimentResult model added
- `results/` - Directory for experiment outputs (gitignored)
- `experiment_errors.log` - Error log file (project root)

### Testing Requirements
**Testing Standards** [Source: architecture/simple-testing-approach.md]
- Use pytest framework with pytest-asyncio for async tests
- Focus on integration tests for full experiment flow
- Mock external API calls to avoid costs during testing
- Test scenarios to implement:
  - Complete experiment execution from start to finish
  - Error handling at various stages (strategy collection, game execution)
  - Partial result saving when experiment fails
  - Proper file structure creation and JSON serialization
  - Graceful shutdown on keyboard interrupt

### Technical Constraints
- Python 3.11+ required [Source: architecture/tech-stack.md]
- Must handle Ctrl+C gracefully to save partial results
- All file writes must be atomic to prevent corruption
- Progress indicators should update in real-time without overwhelming console
- Estimated experiment cost: ~$5 per complete run [Source: architecture/external-apis.md]
- Must respect OpenRouter API rate limits (60 requests/minute for free tier)
- Experiment should complete in approximately 10-15 minutes

### Rate Limiting Implementation
**Rate Limiter Pattern** for OpenRouter API compliance:
```python
import asyncio
from datetime import datetime, timedelta

class RateLimiter:
    """Simple rate limiter for API calls"""
    def __init__(self, max_calls: int = 60, window_seconds: int = 60):
        self.max_calls = max_calls
        self.window = timedelta(seconds=window_seconds)
        self.calls = []

    async def acquire(self):
        """Wait if necessary to respect rate limits"""
        now = datetime.now()
        # Remove calls outside the current window
        self.calls = [call_time for call_time in self.calls
                     if now - call_time < self.window]

        if len(self.calls) >= self.max_calls:
            # Wait until the oldest call expires
            sleep_time = (self.calls[0] + self.window - now).total_seconds()
            await asyncio.sleep(sleep_time)
            return await self.acquire()

        self.calls.append(now)
```

### Project Structure Notes
The project follows a simple structure optimized for research experiments. The main runner script (run_experiment.py) will be placed in the project root alongside test files. All experiment outputs will be saved to the results/ directory which is gitignored to prevent accidental commits of experiment data.

## Testing
- Test file location: Create new test_run_experiment.py in project root
- Test framework: pytest with pytest-asyncio
- Testing patterns: Mock ExperimentFlow and DataManager for fast tests
- Specific test cases required:
  1. Full experiment execution with mocked nodes
  2. Error handling during different phases
  3. Partial result saving on failure
  4. File structure creation and JSON formatting
  5. Graceful shutdown handling

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-30 | 1.0 | Initial story creation | Bob (SM) |

## Dev Agent Record
### Agent Model Used
claude-opus-4-20250514

### Debug Log References
- Created DataManager class with atomic file writes using tempfile
- Integrated DataManager into experiment flow for round-by-round persistence
- Added RateLimiter to respect OpenRouter API limits (60 req/min)
- Implemented progress tracking with moving average time estimation
- Added graceful shutdown handling with signal handlers
- Created comprehensive integration tests with mocked API

### Completion Notes List
- All tasks completed successfully
- DataManager implements atomic writes to prevent data corruption
- Experiment runner integrates data persistence after each round
- Progress tracking shows real-time cooperation rates and time estimates
- Error handling saves partial results and logs all errors
- Integration tests cover all major scenarios including failure cases

### File List
- src/data_manager.py (new)
- run_experiment.py (new)
- test_run_experiment.py (new)

## QA Results

### Review Date: 2025-01-30

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

The implementation demonstrates high-quality code with proper separation of concerns, comprehensive error handling, and well-structured testing. The DataManager class correctly implements atomic file writes using tempfile to prevent data corruption. The ExperimentRunner orchestrates the complete experiment flow with proper progress tracking, rate limiting, and graceful shutdown handling. The integration tests provide good coverage of all major scenarios.

### Refactoring Performed

- **File**: run_experiment.py
  - **Change**: Fixed incorrect method calls to ExperimentFlow instance methods
  - **Why**: The code was trying to call _calculate_asymmetric_cooperation and _calculate_strategy_convergence on the ExperimentFlow instance, but these methods needed to be part of ExperimentRunner
  - **How**: Moved the calculation methods from ExperimentFlow to ExperimentRunner class where they can access the result data properly. This improves encapsulation and ensures the methods have access to all necessary data.

### Compliance Check

- Coding Standards: ✓ All Python files follow PEP 8 conventions, have proper docstrings, and type hints where appropriate
- Project Structure: ✓ Files placed correctly according to project structure (DataManager in src/, runner in root, tests in root)
- Testing Strategy: ✓ Comprehensive integration tests with mocked API, error scenarios, and edge cases
- All ACs Met: ✓ All acceptance criteria fully implemented and tested

### Improvements Checklist

[x] Fixed method location issue for acausal indicator calculations (run_experiment.py)
[x] All atomic write operations properly implemented with tempfile
[x] Rate limiting correctly implemented with proper async handling
[x] Progress tracking with time estimation working as specified
[x] Error handling saves partial results and logs appropriately

### Security Review

No security concerns found. The implementation:
- Does not expose API keys in logs or output
- Uses environment variables for sensitive configuration
- Implements proper file permissions through standard library functions
- No hardcoded credentials or sensitive data

### Performance Considerations

The implementation is well-optimized:
- Atomic writes prevent file corruption without significant overhead
- Rate limiter efficiently manages API calls within limits
- Progress tracking uses lightweight moving average calculation
- Async implementation ensures non-blocking I/O operations

### Final Status

✓ Approved - Ready for Done
