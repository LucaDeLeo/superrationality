# Story 5.2: Strategy Similarity Computation

## Status
Done

## Story
**As a** researcher,
**I need to** measure how similar agent strategies are,
**So that** I can quantify convergence on cooperative strategies

## Acceptance Criteria
1. Cosine similarity computed across strategy vectors
2. Similarity tracked across rounds
3. Visualization of strategy clustering

## Tasks / Subtasks
- [x] Task 1: Create SimilarityNode class structure (AC: 1, 2)
  - [x] Subtask 1.1: Create src/nodes/similarity.py with SimilarityNode class extending AsyncNode from src.nodes.base
  - [x] Subtask 1.2: Define strategy vectorization approach (TF-IDF, word embeddings, or semantic hashing)
  - [x] Subtask 1.3: Implement configuration for similarity computation parameters
  - [x] Subtask 1.4: Implement execute() method to orchestrate similarity analysis workflow
- [x] Task 2: Implement strategy vectorization (AC: 1)
  - [x] Subtask 2.1: Create vectorize_strategy() method to convert strategy text to numerical vectors
  - [x] Subtask 2.2: Implement TF-IDF vectorization using scikit-learn or similar library
  - [x] Subtask 2.3: Handle vocabulary building across all strategies in experiment
  - [x] Subtask 2.4: Normalize vectors for cosine similarity computation
  - [x] Subtask 2.5: Add fallback for empty or very short strategies
- [x] Task 3: Compute pairwise similarity matrices (AC: 1, 2)
  - [x] Subtask 3.1: Create compute_similarity_matrix() method for single round
  - [x] Subtask 3.2: Implement efficient cosine similarity computation using numpy/scipy
  - [x] Subtask 3.3: Store similarity matrices for each round
  - [x] Subtask 3.4: Calculate average similarity per round
  - [x] Subtask 3.5: Track similarity evolution across rounds
- [x] Task 4: Implement clustering analysis (AC: 3)
  - [x] Subtask 4.1: Create identify_strategy_clusters() method using similarity data
  - [x] Subtask 4.2: Implement hierarchical clustering or K-means on similarity matrices
  - [x] Subtask 4.3: Determine optimal number of clusters using elbow method or silhouette score
  - [x] Subtask 4.4: Track cluster membership changes across rounds
  - [x] Subtask 4.5: Generate cluster descriptions based on common strategy patterns
- [x] Task 5: Generate similarity analysis output (AC: 1, 2, 3)
  - [x] Subtask 5.1: Create generate_similarity_report() method to compile results
  - [x] Subtask 5.2: Calculate convergence metrics (e.g., increasing average similarity over rounds)
  - [x] Subtask 5.3: Prepare visualization data for strategy clustering (coordinates for plotting)
  - [x] Subtask 5.4: Generate summary statistics on similarity trends
  - [x] Subtask 5.5: Save analysis to strategy_similarity.json using DataManager._write_json()
  - [x] Subtask 5.6: Include metadata about vectorization method and parameters
- [x] Task 6: Integrate with experiment flow (AC: 1, 2, 3)
  - [x] Subtask 6.1: Add SimilarityNode instantiation and execution in ExperimentFlow.run() after AnalysisNode
  - [x] Subtask 6.2: Pass strategy data from AnalysisNode context
  - [x] Subtask 6.3: Update experiment_summary.json with similarity metrics by adding to result.acausal_indicators
  - [x] Subtask 6.4: Ensure proper error handling and logging
- [x] Task 7: Create comprehensive unit tests (AC: 1-3)
  - [x] Subtask 7.1: Test strategy vectorization with various input types
  - [x] Subtask 7.2: Test cosine similarity computation accuracy
  - [x] Subtask 7.3: Test clustering algorithm with known patterns
  - [x] Subtask 7.4: Test similarity tracking across multiple rounds
  - [x] Subtask 7.5: Test visualization data generation
  - [x] Subtask 7.6: Test edge cases (single strategy, identical strategies, no strategies)
  - [x] Subtask 7.7: Test integration with experiment flow

## Dev Notes

### Previous Story Insights
From Story 5.1 (Analysis Node):
- AsyncNode pattern established and working well
- DataManager's _write_json method provides atomic file operations
- Strategy files loaded from strategies_r{N}.json with full text in "strategy" field
- Analysis results saved in similar JSON format with metadata
- Experiment flow runs analysis nodes after all tournament rounds complete

### Data Models
**Strategy Data Format** [Source: database-schema.md#strategies_r{N}.json]
```json
{
  "round": 1,
  "timestamp": "2024-01-15T10:30:00Z",
  "strategies": [
    {
      "strategy_id": "r1_a0_1234567890",
      "agent_id": 0,
      "round": 1,
      "strategy_text": "Always cooperate if opponent cooperated last time",
      "full_reasoning": "Given that we are all identical agents...",
      "timestamp": "2024-01-15T10:30:15Z"
    }
  ]
}
```

**Expected Similarity Output Format** [Source: Epic 5 PRD and database-schema.md#acausal_analysis.json]
```json
{
  "strategy_similarity_analysis": {
    "experiment_id": "exp_20240115_103000",
    "analysis_timestamp": "2024-01-15T11:50:00Z",
    "rounds_analyzed": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    "similarity_by_round": {
      "1": {
        "average_similarity": 0.67,
        "similarity_matrix": [[1.0, 0.72, ...], [0.72, 1.0, ...], ...],
        "min_similarity": 0.45,
        "max_similarity": 0.89
      },
      "2": { ... },
      ...
    },
    "convergence_metrics": {
      "strategy_convergence": 0.81,
      "rounds_to_convergence": 4,
      "convergence_trend": [0.67, 0.71, 0.75, 0.81, 0.82, 0.83, 0.84, 0.84, 0.85, 0.85]
    },
    "clustering_analysis": {
      "optimal_clusters": 3,
      "cluster_descriptions": {
        "0": "Cooperative strategies with identity reasoning",
        "1": "Conditional cooperation based on power",
        "2": "Defection-oriented strategies"
      },
      "cluster_evolution": {
        "1": {"0": [0, 2, 5], "1": [1, 3, 7, 8], "2": [4, 6, 9]},
        "2": { ... },
        ...
      }
    },
    "visualization_data": {
      "strategy_embeddings_2d": {
        "1": [[0.12, 0.45], [0.34, 0.21], ...],  // Round 1 2D coordinates
        "2": [[0.15, 0.48], [0.36, 0.19], ...],  // Round 2 2D coordinates
        ...
      }
    },
    "metadata": {
      "vectorization_method": "tfidf",
      "similarity_metric": "cosine",
      "clustering_algorithm": "hierarchical",
      "parameters": {
        "tfidf_max_features": 100,
        "tfidf_ngram_range": [1, 2],
        "clustering_linkage": "average"
      }
    }
  }
}
```

### Output File Strategy

The similarity analysis will produce a single output file:
- **File**: `results/{experiment_id}/strategy_similarity.json`
- **Format**: JSON containing complete similarity analysis results
- **Contents**: All similarity metrics, clustering analysis, and visualization data as shown in the Expected Output Format
- **Method**: Save using `DataManager._write_json("strategy_similarity.json", analysis_results)`
- **Integration**: The experiment_summary.json will reference key metrics from this analysis

### Technical Details

**Vectorization Approach:**
1. TF-IDF (Term Frequency-Inverse Document Frequency) for initial implementation
2. Consider all strategies across all rounds as document corpus
3. Use bi-grams (1-2 word phrases) to capture strategy patterns
4. Normalize vectors to unit length for cosine similarity

**Similarity Computation:**
- Use scipy.spatial.distance.cosine for pairwise similarity
- Create NxN similarity matrix for each round (N=10 agents)
- Average similarity = mean of upper triangle (excluding diagonal)

**Clustering Approach:**
- Hierarchical clustering with average linkage
- Use similarity matrix as distance matrix (distance = 1 - similarity)
- Determine optimal clusters using dendrogram analysis
- Alternative: K-means on strategy vectors if hierarchical doesn't work well

**Visualization Data:**
- Use dimensionality reduction (PCA or t-SNE) to create 2D coordinates
- Preserve relative distances from similarity matrix
- Generate coordinates for each round to show evolution

### File Locations
- `src/nodes/similarity.py` - New SimilarityNode class
- `src/flows/experiment.py` - Update to include SimilarityNode in analysis phase
- `test_similarity.py` - New test file in project root
- Output: `results/{experiment_id}/strategy_similarity.json` using DataManager

### Technical Constraints
- Must handle experiments with 100 strategies (10 agents Ã— 10 rounds)
- Similarity computation should complete within 10 seconds
- Memory usage should stay under 200MB
- Must use libraries already in project or commonly available (numpy, scipy, scikit-learn)

### Integration Points
1. **ExperimentFlow**: Add SimilarityNode after AnalysisNode in run() method
   ```python
   # In ExperimentFlow.run() after line 325 (analysis_node execution)
   similarity_node = SimilarityNode()
   context = await similarity_node.execute(context)

   # Extract similarity results
   similarity_analysis = context.get("similarity_analysis", {})
   ```
2. **Context Passing**:
   - Input: DataManager from context[ContextKeys.DATA_MANAGER]
   - Input: experiment_id from context[ContextKeys.EXPERIMENT_ID]
   - Output: Add "similarity_analysis" to context
3. **DataManager**: Use for loading strategy files and saving similarity analysis
   - Load: Similar to AnalysisNode - use `rounds_path.glob("strategies_r*.json")` pattern
   - Parse: Extract round number from filename with regex
   - Save: `data_manager._write_json(Path("strategy_similarity.json"), results)`
4. **Error Handling**: Continue with partial results if some rounds missing

### Dependencies
**Required Python packages:**
- numpy: For array operations and similarity computation
- scipy: For spatial distance calculations and clustering
- scikit-learn: For TF-IDF vectorization

**Installation:**
Add to requirements.txt:
```
numpy>=1.24.0
scipy>=1.10.0
scikit-learn>=1.3.0
```

Then install:
```bash
pip install -r requirements.txt
```

**Verification:**
Before implementation, verify these packages are compatible with the project's Python version and existing dependencies (aiohttp, pytest, etc.). The scientific computing packages (numpy, scipy, scikit-learn) are mature and widely compatible.

### Implementation Architecture
```python
from src.nodes.base import AsyncNode, ContextKeys
from src.utils.data_manager import DataManager
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from scipy.spatial.distance import cosine
from scipy.cluster.hierarchy import linkage, fcluster
from typing import Dict, Any, List

class SimilarityNode(AsyncNode):
    """Computes similarity between agent strategies across rounds."""

    def __init__(self):
        """Initialize SimilarityNode with vectorizer and results structure."""
        super().__init__(max_retries=1)  # Don't retry analysis failures

        self.vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            stop_words='english'
        )

    async def _execute_impl(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute similarity analysis on strategies from all rounds."""
        # Get data manager and experiment ID from context
        data_manager = context.get(ContextKeys.DATA_MANAGER)
        experiment_id = context.get(ContextKeys.EXPERIMENT_ID)

        # Load strategies, compute similarity, save results
        # Add "similarity_analysis" to context
        return context
```

## Testing
- Test file location: `test_similarity.py` in project root
- Test framework: pytest with pytest-asyncio
- Test runner command: `pytest test_similarity.py -v`
- Specific test cases required:
  1. test_strategy_vectorization
  2. test_tfidf_with_empty_strategies
  3. test_cosine_similarity_computation
  4. test_similarity_matrix_properties
  5. test_convergence_metric_calculation
  6. test_clustering_identification
  7. test_cluster_evolution_tracking
  8. test_visualization_data_generation
  9. test_load_strategies_from_files
  10. test_handle_missing_rounds
  11. test_similarity_report_generation
  12. test_integration_with_experiment
  13. test_identical_strategies_similarity
  14. test_completely_different_strategies
  15. test_metadata_generation

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-31 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-01-31 | 1.1 | Completed Tasks 1-4, created test suite | James (Dev) |
| 2025-02-01 | 1.2 | Completed Tasks 5-7, fixed path bug, all tests passing | James (Dev) |

## Dev Agent Record
_To be filled by development agent_

### Agent Model Used
claude-opus-4-20250514

### Debug Log References
- Created test_similarity.py with comprehensive unit tests
- Fixed TF-IDF vectorizer issues with small document sets
- Fixed DataManager._write_json path issue in similarity.py (needed absolute path)
- Added test_integration_with_experiment test case
- Verified all 19 tests passing

### Completion Notes List
- Task 1: SimilarityNode class created with all required functionality including TF-IDF vectorization configuration
- Task 2: Strategy vectorization implemented using TfidfVectorizer with ngram_range=(1,2) and proper normalization
- Task 3: Pairwise cosine similarity computation implemented with efficient matrix operations
- Task 4: Hierarchical clustering with silhouette score optimization implemented
- Task 5: Generate similarity report method already implemented with all required features (convergence metrics, visualization data, metadata)
- Task 6: Integration with experiment flow VERIFIED COMPLETE - SimilarityNode is imported at line 343, instantiated and executed at lines 345-347 after AnalysisNode, similarity results extracted at line 350, proper error handling implemented (lines 352-359), and convergence metrics integrated into acausal_indicators (lines 432-436)
- Task 7: Created comprehensive test suite (test_similarity.py) with 19 tests covering all functionality including integration test
- Fixed bug: Updated _save_results to use absolute path for DataManager._write_json

### File List
- src/nodes/similarity.py (modified - fixed _save_results method to use absolute path)
- test_similarity.py (modified - added test_integration_with_experiment test case)

## QA Results

### Review Date: 2025-02-02

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

**Overall Assessment**: Excellent implementation with production-ready quality. The code demonstrates senior-level architectural decisions and comprehensive error handling throughout.

### Code Architecture Review

The implementation follows a clean, well-structured architecture:
- **SimilarityNode** class properly extends AsyncNode from base with appropriate retry configuration
- Follows established patterns from AnalysisNode (Story 5.1) maintaining consistency
- Clear separation of concerns with dedicated methods for each analysis phase
- Proper async/await patterns throughout
- Comprehensive error handling and logging at all critical points

### Refactoring Performed

After thorough review, no refactoring was necessary. The implementation already demonstrates:
- Clean code principles with proper method decomposition
- Efficient algorithms using numpy/scipy for performance
- Proper error boundaries and graceful degradation
- Well-structured data flow and transformations

### Compliance Check

- Coding Standards: âœ“ Follows PEP 8 and project conventions
- Project Structure: âœ“ Files correctly placed according to unified-project-structure.md
- Testing Strategy: âœ“ Comprehensive test suite with 19 test cases
- All ACs Met: âœ“ All acceptance criteria fully implemented and tested

### Implementation Quality Analysis

**Task 1: SimilarityNode Class Structure** âœ“
- Clean initialization with proper configuration parameters
- Inherits from AsyncNode correctly with max_retries=1
- Well-organized instance variables for storing analysis state

**Task 2: Strategy Vectorization** âœ“
- TF-IDF properly configured with ngram_range=(1,2)
- Excellent handling of edge cases (empty strategies)
- Correct vector normalization for cosine similarity
- Proper handling of small document sets with min_df=1

**Task 3: Pairwise Similarity Computation** âœ“
- Efficient matrix computation avoiding redundant calculations
- Properly symmetric matrices with diagonal = 1.0
- Correct implementation: `sim = 1.0 - cosine(vectors[i], vectors[j])`

**Task 4: Clustering Analysis** âœ“
- Hierarchical clustering with average linkage correctly implemented
- Dynamic cluster optimization using silhouette scores
- Proper distance matrix conversion handling
- Edge cases handled (insufficient data for clustering)

**Task 5: Report Generation** âœ“
- Output format matches specification exactly
- All required metrics calculated correctly
- Proper metadata tracking of analysis parameters
- Clean JSON structure for easy consumption

**Task 6: Integration** âœ“
- Correctly integrated into ExperimentFlow at lines 343-359
- Properly extracts convergence metrics for acausal_indicators (lines 432-436)
- Uses absolute paths for DataManager operations
- Proper error handling with fallback to empty analysis

**Task 7: Test Coverage** âœ“
- Comprehensive test suite with all required test cases plus extras
- Edge cases properly tested
- Integration test verifies end-to-end functionality
- All 19 tests passing

### Improvements Checklist

All implementation requirements have been met. No improvements required at this time.

### Security Review

- No security vulnerabilities identified
- Proper input validation on all user data
- Safe file operations through DataManager abstraction
- No hardcoded credentials or sensitive data

### Performance Considerations

- Memory efficient: Processes rounds individually
- Time complexity O(nÂ²) for similarity matrix is acceptable for 10 agents
- Proper use of numpy/scipy for optimized operations
- Scalable design can handle missing rounds gracefully

### Final Status

âœ“ Approved - Ready for Done

The implementation is production-ready with excellent code quality, comprehensive testing, and seamless integration. The developer has successfully implemented all requirements with proper attention to edge cases and performance considerations.
