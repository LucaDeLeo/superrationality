# Story 6.3: Cross-Model Cooperation Analysis

## Status
Ready for Review

## Story
**As a** researcher,
**I need** to analyze cooperation patterns between different model types,
**So that** I can identify if models cooperate more with their own type

## Acceptance Criteria
1. Analysis tracks cooperation rates by model pairing
2. Identifies "in-group" vs "out-group" cooperation patterns
3. Detects model-specific reasoning patterns
4. Compares cooperation rates: same-model vs cross-model
5. Generates model interaction heatmaps

## Tasks / Subtasks
- [x] Task 1: Create CrossModelAnalyzer utility class (AC: 1, 2, 4, 5)
  - [x] Subtask 1.1: Create new file `src/utils/cross_model_analyzer.py` with CrossModelAnalyzer class
  - [x] Subtask 1.2: Import necessary dependencies (pandas, numpy, defaultdict, etc.)
  - [x] Subtask 1.3: Define class structure with init method to store game results and strategy records
  - [x] Subtask 1.4: Create helper method to join GameResult data with StrategyRecord model information
  - [x] Subtask 1.5: Add error handling for missing model data or incomplete joins
  - [x] Subtask 1.6: Write unit tests in `test_cross_model_analyzer.py` (project root)
- [x] Task 2: Implement cooperation matrix calculation (AC: 1, 4)
  - [x] Subtask 2.1: Add calculate_cooperation_matrix() method to CrossModelAnalyzer
  - [x] Subtask 2.2: Join GameResult with StrategyRecord to get model types for each player
  - [x] Subtask 2.3: Build pandas DataFrame with rows/cols as model types, values as cooperation rates
  - [x] Subtask 2.4: Handle missing model pairings gracefully (return NaN)
  - [x] Subtask 2.5: Calculate cooperation counts and total games per model pairing
  - [x] Subtask 2.6: Add statistical significance calculations (chi-square, confidence intervals)
  - [x] Subtask 2.7: Write comprehensive unit tests for matrix calculation
- [x] Task 3: Implement in-group vs out-group cooperation analysis (AC: 2, 4)
  - [x] Subtask 3.1: Add detect_in_group_bias() method to CrossModelAnalyzer
  - [x] Subtask 3.2: Calculate same-model cooperation rate (e.g., GPT-4 vs GPT-4)
  - [x] Subtask 3.3: Calculate cross-model cooperation rate (e.g., GPT-4 vs Claude-3)
  - [x] Subtask 3.4: Compute in-group bias as difference between same-model and cross-model rates
  - [x] Subtask 3.5: Add statistical significance testing for bias (t-test, effect size)
  - [x] Subtask 3.6: Track confidence intervals and sample sizes for bias calculations
  - [x] Subtask 3.7: Create unit tests for bias detection with edge cases
- [x] Task 4: Enhance model-specific pattern detection in AnalysisNode (AC: 3)
  - [x] Subtask 4.1: Update marker_patterns in AnalysisNode.__init__ to include model-specific markers
  - [x] Subtask 4.2: Add GPT-4 specific patterns: "chain of thought", "explicit utility calculation"
  - [x] Subtask 4.3: Add Claude specific patterns: "constitutional principles", "harm minimization"
  - [x] Subtask 4.4: Add Gemini specific patterns: analytical/structured reasoning markers
  - [x] Subtask 4.5: Extend _track_model_specific_pattern() to capture these new patterns
  - [x] Subtask 4.6: Update analyze_transcript() to properly categorize model-specific behaviors
  - [x] Subtask 4.7: Write tests for new pattern detection in test_analysis_model_specific.py
- [x] Task 5: Implement model coalition detection (AC: 2, 3)
  - [x] Subtask 5.1: Add analyze_model_coalitions() method to CrossModelAnalyzer
  - [x] Subtask 5.2: Track persistent cooperation clusters across rounds by model pairing
  - [x] Subtask 5.3: Calculate coalition strength scores based on consistency over time
  - [x] Subtask 5.4: Identify strongest and weakest cooperation pairs
  - [x] Subtask 5.5: Generate coalition network data for visualization
  - [x] Subtask 5.6: Write tests for coalition detection algorithms
- [x] Task 6: Create visualization data structures (AC: 5)
  - [x] Subtask 6.1: Add generate_heatmap_data() method to generate JSON-serializable heatmap structure
  - [x] Subtask 6.2: Implement color scaling based on cooperation rates (0-1 scale)
  - [x] Subtask 6.3: Include statistical significance indicators in visualization data
  - [x] Subtask 6.4: Create time-series data showing cooperation evolution by model pairing
  - [x] Subtask 6.5: Export visualization data in format suitable for matplotlib/seaborn
  - [x] Subtask 6.6: Write tests for visualization data generation
- [x] Task 7: Integrate cross-model analysis into AnalysisNode workflow (AC: 1-5)
  - [x] Subtask 7.1: Import CrossModelAnalyzer in AnalysisNode
  - [x] Subtask 7.2: Load GameResult data in addition to strategy files in _execute_impl()
  - [x] Subtask 7.3: Initialize CrossModelAnalyzer with game results and strategy records
  - [x] Subtask 7.4: Call all CrossModelAnalyzer methods during analysis workflow
  - [x] Subtask 7.5: Add cross_model_analysis section to generate_analysis_report()
  - [x] Subtask 7.6: Include cooperation matrix, in-group bias, coalitions, and visualizations in report
  - [x] Subtask 7.7: Ensure backward compatibility with single-model experiments
  - [x] Subtask 7.8: Write integration tests for complete cross-model analysis flow
- [x] Task 8: Add comprehensive cross-model statistics (AC: 1, 2, 4)
  - [x] Subtask 8.1: Calculate average cooperation rate per model type
  - [x] Subtask 8.2: Compute model diversity impact on overall cooperation
  - [x] Subtask 8.3: Add sample size warnings for low-data model pairings
  - [x] Subtask 8.4: Track emergence of model-specific coalitions over rounds
  - [x] Subtask 8.5: Calculate statistical power for each model pairing comparison
  - [x] Subtask 8.6: Create comprehensive test suite for all metrics

## Dev Notes

### Previous Story Insights
Story 6.2 successfully implemented model-specific strategy collection with:
- Model type tracked in StrategyRecord.model field
- Model-specific response parsing for different formats
- Enhanced AnalysisNode with basic model tracking:
  - `strategies_by_model` defaultdict tracking counts
  - `markers_by_model` defaultdict tracking marker counts by model
  - `model_specific_patterns` defaultdict tracking unique patterns
- Model version and parameters captured in metadata
[Source: docs/stories/6.2.story.md#Dev-Agent-Record]

### Current Analysis Implementation
The AnalysisNode already has foundations for model-specific tracking:
```python
self.analysis_results = {
    # ... existing fields ...
    "strategies_by_model": defaultdict(int),  # Count strategies by model
    "markers_by_model": defaultdict(lambda: defaultdict(int)),  # Markers by model
    "model_specific_patterns": defaultdict(list),  # Model-specific patterns
}
```
[Source: src/nodes/analysis.py#L103-105]

The analyze_transcript method already extracts model information:
```python
model = strategy_data.get("model", "unknown")  # Get model type
self.analysis_results["strategies_by_model"][model] += 1
self.analysis_results["markers_by_model"][model][category] += 1
```
[Source: src/nodes/analysis.py#L217,226,240]

### Data Models
GameResult stores game outcomes but needs model information added through join with StrategyRecord:
```python
@dataclass
class GameResult:
    game_id: str
    round: int
    player1_id: int
    player2_id: int
    player1_action: str  # COOPERATE or DEFECT
    player2_action: str  # COOPERATE or DEFECT
    # ... other fields
```
[Source: docs/architecture/data-models.md#GameResult]

StrategyRecord contains model information:
```python
@dataclass 
class StrategyRecord:
    agent_id: int
    model: str  # Model type stored here
    # ... other fields
```
[Source: src/core/models.py#StrategyRecord]

### Cross-Model Analysis Components
From Epic 6 specifications, we need to implement:
```python
class CrossModelAnalyzer:
    def calculate_cooperation_matrix(self, games: List[Game]) -> pd.DataFrame:
        """Generate NxN matrix of cooperation rates between model types"""
        
    def detect_in_group_bias(self, games: List[Game]) -> Dict[str, float]:
        """Calculate cooperation rate difference: same-model vs different-model"""
        
    def analyze_model_coalitions(self, tournament_data: TournamentData) -> CoalitionReport:
        """Detect if models form implicit coalitions"""
```
[Source: docs/prd/epic-6-multi-model-experiments.md#L133-156]

### File Locations
Based on project structure and existing patterns:
- `src/utils/cross_model_analyzer.py` - NEW file for CrossModelAnalyzer class
- `src/nodes/analysis.py` - Extend existing AnalysisNode
- `test_cross_model_analyzer.py` - NEW test file in project root (following existing test file patterns)
[Source: Project structure inspection and test file location patterns]

### Technical Constraints
- Must join GameResult with StrategyRecord to get model types for each player
- Handle experiments where model information might be missing (backward compatibility)
- Cooperation matrix should use pandas DataFrame for easy manipulation
- Statistical calculations should handle small sample sizes appropriately
- Memory efficient for large multi-model experiments
[Source: docs/architecture/backend-architecture.md#Performance-Considerations]

### Model-Specific Behavioral Patterns
Epic 6 specifies these model-specific markers to detect:
- GPT-4: "chain of thought", "explicit utility calculation"
- Claude-3: "constitutional principles", "harm minimization"  
- Gemini: analytical, structured reasoning patterns
These should be added to the marker_patterns in AnalysisNode
[Source: docs/prd/epic-6-multi-model-experiments.md#L206-215]

### Output Format Extensions
The analysis should add this structure to the report:
```json
{
  "cross_model_analysis": {
    "cooperation_matrix": {
      "gpt-4": {"gpt-4": 0.95, "claude-3": 0.68},
      "claude-3": {"gpt-4": 0.67, "claude-3": 0.94}
    },
    "in_group_bias": 0.27,
    "model_statistics": {
      "gpt-4": {"avg_cooperation": 0.82, "total_games": 450},
      "claude-3": {"avg_cooperation": 0.79, "total_games": 450}
    },
    "strongest_cooperation_pair": ["gpt-4", "gpt-4"],
    "weakest_cooperation_pair": ["gemini-pro", "claude-3"],
    "model_diversity_impact": -0.15,
    "model_coalitions": {
      "detected": false,
      "coalition_groups": []
    },
    "visualization_data": {
      "heatmap": {...},
      "time_series": {...}
    }
  }
}
```
[Source: docs/prd/epic-6-multi-model-experiments.md#L194-203]

## Testing
- Test file location: `test_cross_model_analyzer.py` in project root (following existing pattern)
- Use pytest framework following existing test patterns
- Mock game results and strategy records for different model combinations
- Test coverage requirements:
  - Cooperation matrix calculation with various model distributions
  - In-group bias detection with edge cases (single model, no games, etc.)
  - Model-specific pattern detection in AnalysisNode
  - Coalition detection algorithms
  - Visualization data generation
  - Integration with existing analysis flow
  - Backward compatibility with single-model experiments
  - Statistical significance calculations

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-02 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-08-02 | 2.0 | Revised to resolve architectural ambiguities and add missing tasks | Bob (Scrum Master) |

## Dev Agent Record
_To be filled by development agent_

### Agent Model Used
claude-opus-4-20250514

### Debug Log References
None

### Completion Notes List
- Completed Tasks 1-5 as requested
- Created CrossModelAnalyzer utility class with full implementation of cooperation matrix, in-group bias detection, and coalition analysis
- Enhanced AnalysisNode with model-specific pattern detection for GPT-4, Claude, and Gemini models
- Implemented comprehensive test suites for all new functionality
- All methods include proper error handling and statistical significance calculations
- Coalition detection tracks cooperation patterns across rounds and calculates strength scores
- Completed Tasks 6-8: Added visualization data generation, integrated cross-model analysis into AnalysisNode workflow, and implemented comprehensive cross-model statistics
- Added generate_heatmap_data() method with JSON-serializable output including heatmap matrix, time series, and significance data
- Integrated CrossModelAnalyzer into AnalysisNode with game file loading and backward compatibility for single-model experiments
- Implemented comprehensive statistics: average cooperation by model, diversity impact, sample size warnings, coalition emergence tracking, and statistical power calculations
- Added extensive integration tests for the complete workflow

### File List
- src/utils/cross_model_analyzer.py (NEW) - Expanded with visualization and comprehensive statistics methods
- test_cross_model_analyzer.py (NEW) - Expanded with tests for all new functionality
- src/nodes/analysis.py (MODIFIED) - Integrated cross-model analysis workflow
- test_analysis_model_specific.py (NEW)
- test_analysis.py (MODIFIED) - Added integration tests for cross-model analysis

## Next Developer Context

This story builds on the model tracking infrastructure from Stories 6.1 and 6.2. The AnalysisNode already has basic model tracking, but needs to be extended with sophisticated cross-model cooperation analysis.

Key implementation decisions made in this revision:
1. **Architecture Decision**: Create a separate CrossModelAnalyzer utility class rather than extending AnalysisNode directly. This follows separation of concerns and makes the code more maintainable.
2. **Data Join Implementation**: Added explicit task (Task 2.2) to join GameResult with StrategyRecord to get model information.
3. **File Creation Clarity**: Clearly specified all file locations following existing project patterns.
4. **Statistical Rigor**: Added explicit subtasks for statistical significance testing throughout.

Critical implementation notes:
1. The cooperation matrix is the core deliverable - an NxN pandas DataFrame showing cooperation rates
2. The data join between GameResult and StrategyRecord is critical for determining which models played against each other
3. In-group bias calculation will reveal if models prefer cooperating with their own type
4. Model coalitions detection will show if certain model pairs consistently cooperate
5. All analysis must handle missing model data gracefully for backward compatibility
6. Test files go in the project root, following the existing pattern

The next story (6.4) will likely focus on mixed model scenario testing, so ensure the analysis infrastructure can handle arbitrary model distributions.

## Next Developer Context for Remaining Tasks

### Completed Work (Tasks 1-5)
I have successfully implemented the core cross-model cooperation analysis infrastructure:

1. **CrossModelAnalyzer Class** (`src/utils/cross_model_analyzer.py`):
   - `load_data()`: Loads game results and strategy records, builds agent->model mapping
   - `calculate_cooperation_matrix()`: Creates pandas DataFrame with cooperation rates between all model pairs
   - `get_cooperation_stats()`: Provides detailed statistics with confidence intervals
   - `detect_in_group_bias()`: Calculates bias between same-model vs cross-model cooperation with statistical significance
   - `analyze_model_coalitions()`: Detects persistent cooperation patterns across rounds

2. **Enhanced AnalysisNode** (`src/nodes/analysis.py`):
   - Added model-specific pattern detection for GPT-4, Claude, and Gemini
   - Patterns are now tracked separately by model type
   - Model insights are generated in the final report

3. **Comprehensive Test Coverage**:
   - `test_cross_model_analyzer.py`: Tests all CrossModelAnalyzer functionality
   - `test_analysis_model_specific.py`: Tests model-specific pattern detection

### Remaining Tasks (6-8)

**Task 6: Create visualization data structures**
- Need to implement `generate_heatmap_data()` method in CrossModelAnalyzer
- Should return JSON-serializable format suitable for matplotlib/seaborn
- Consider using a structure like:
```python
{
    "matrix": [[0.95, 0.68], [0.67, 0.94]],  # Cooperation rates
    "labels": ["gpt-4", "claude-3"],          # Model names
    "significance": [[1.0, 0.001], [0.001, 1.0]],  # P-values
    "color_scale": {"min": 0, "max": 1, "midpoint": 0.5}
}
```

**Task 7: Integrate cross-model analysis into AnalysisNode workflow**
- The AnalysisNode needs to load game results in addition to strategy files
- Initialize CrossModelAnalyzer in `_execute_impl()`
- Add cross-model analysis section to the final report
- Key integration point is around line 136 in analysis.py

**Task 8: Add comprehensive cross-model statistics**
- Calculate model diversity impact on cooperation
- Add warnings for low sample sizes
- Track statistical power for comparisons

### Implementation Notes

1. **Data Loading in AnalysisNode**: 
   - Game results are stored in `rounds/games_r*.json` files
   - Need to load these alongside strategy files in `load_strategy_files()` or create a new method

2. **Report Structure**:
   - The cross-model analysis should be added as a new section in `generate_analysis_report()`
   - Follow the structure outlined in the Dev Notes (lines 178-205)

3. **Backward Compatibility**:
   - Ensure the analysis still works for single-model experiments
   - Check if model information exists before running cross-model analysis

4. **Testing Remaining Tasks**:
   - Add integration tests for the complete workflow
   - Test visualization data generation with various model distributions
   - Test edge cases like single-model experiments

### Key Files to Modify Next
1. `src/utils/cross_model_analyzer.py` - Add visualization methods
2. `src/nodes/analysis.py` - Integrate CrossModelAnalyzer
3. Test files - Add integration tests

The foundation is solid - the remaining tasks are mainly integration and visualization work.

## QA Results

### Review Date: 2025-08-02

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

The implementation is comprehensive and meets all acceptance criteria. The developer has successfully created a robust cross-model cooperation analysis system that tracks cooperation patterns between different model types, detects in-group biases, and provides visualization capabilities. The code demonstrates strong engineering practices with proper error handling, statistical calculations, and extensive test coverage.

### Refactoring Performed

- **File**: src/utils/cross_model_analyzer.py
  - **Change**: Extracted common cooperation counting logic into `_calculate_cooperation_counts()` helper method
  - **Why**: Code duplication between `calculate_cooperation_matrix()` and `get_cooperation_stats()` violated DRY principle
  - **How**: Reduces maintenance burden and ensures consistency in cooperation calculations

- **File**: src/utils/cross_model_analyzer.py
  - **Change**: Added validation to `_find_coalition_formation_round()` for empty lists and missing keys
  - **Why**: Method could throw exceptions on edge cases with empty or malformed data
  - **How**: Improves robustness by gracefully handling edge cases

- **File**: src/nodes/analysis.py
  - **Change**: Simplified `_calculate_model_statistics()` to use analyzer's existing method
  - **Why**: Method was duplicating logic already present in CrossModelAnalyzer
  - **How**: Reduces code duplication and leverages existing functionality

- **File**: src/nodes/analysis.py
  - **Change**: Added data validation in `_perform_cross_model_analysis()` before processing
  - **Why**: Method could fail silently with empty data sets
  - **How**: Provides early warning and graceful degradation for insufficient data

### Compliance Check

- Coding Standards: ✓ Code follows Python conventions and project patterns
- Project Structure: ✓ Files correctly placed in src/utils/ and appropriate test locations
- Testing Strategy: ✓ Comprehensive test coverage with 1350+ lines of tests
- All ACs Met: ✓ All 5 acceptance criteria fully implemented

### Improvements Checklist

- [x] Refactored cooperation counting to eliminate code duplication
- [x] Added edge case handling for coalition formation detection
- [x] Simplified model statistics calculation
- [x] Added data validation for cross-model analysis
- [ ] Consider adding caching for expensive statistical calculations in future iterations
- [ ] Consider extracting visualization data generation into separate visualizer class
- [ ] Add performance monitoring for large-scale experiments

### Security Review

No security concerns identified. The code properly validates inputs and handles missing data gracefully.

### Performance Considerations

The implementation efficiently processes game data with O(n) complexity for most operations. The statistical calculations use optimized NumPy/Pandas operations. For very large experiments (>10,000 games), consider implementing result caching for the cooperation matrix calculation.

### Final Status

✓ Approved - Ready for Done

The implementation exceeds expectations with robust error handling, comprehensive statistics, and excellent test coverage. The refactoring performed improves code maintainability while preserving all functionality.