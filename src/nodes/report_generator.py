"""Report generator node for creating unified analysis reports."""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import re

from src.nodes.base import AsyncNode, ContextKeys
from src.utils.data_manager import DataManager

logger = logging.getLogger(__name__)


class ReportGeneratorNode(AsyncNode):
    """Generates unified analysis reports from all analysis nodes."""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize ReportGeneratorNode with configuration.
        
        Args:
            config: Configuration dictionary for report generation
        """
        super().__init__(max_retries=1)  # Don't retry report generation
        self.config = config or {}
        
        # Report formats to generate
        self.report_formats = ["json", "markdown", "latex"]
        
        # Configurable acausal score weights
        self.acausal_weights = self.config.get("acausal_weights", {
            "identity_reasoning": 0.3,
            "cooperation_rate": 0.25,
            "strategy_convergence": 0.25,
            "cooperation_trend": 0.2
        })
        
        # Ensure weights sum to 1.0
        weight_sum = sum(self.acausal_weights.values())
        if abs(weight_sum - 1.0) > 0.001:
            logger.warning(f"Acausal weights sum to {weight_sum}, normalizing to 1.0")
            self.acausal_weights = {
                k: v / weight_sum for k, v in self.acausal_weights.items()
            }
        
        # Configurable report sections
        self.enabled_sections = self.config.get("enabled_sections", {
            "executive_summary": True,
            "detailed_findings": True,
            "visualizations": True,
            "latex_sections": True,
            "correlation_analysis": True
        })
        
    async def _execute_impl(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive reports from analysis results.
        
        Args:
            context: Experiment context with analysis results
            
        Returns:
            Updated context with report data
        """
        logger.info("Starting report generation")
        
        # Store context for use in visualization methods
        self._current_context = context
        
        # Extract results from context
        transcript_analysis = context.get("transcript_analysis", {})
        similarity_analysis = context.get("similarity_analysis", {})
        statistical_analysis = context.get("statistical_analysis", {})
        
        # Log what analyses we have
        logger.info(f"Found analyses: transcript={bool(transcript_analysis)}, "
                   f"similarity={bool(similarity_analysis)}, "
                   f"statistics={bool(statistical_analysis)}")
        
        # Synthesize findings across all analyses
        synthesis = self.synthesize_findings(
            transcript_analysis,
            similarity_analysis,
            statistical_analysis
        )
        
        # Generate report components based on enabled sections
        reports = {
            "synthesis": synthesis,
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "experiment_id": context.get(ContextKeys.EXPERIMENT_ID, "unknown"),
                "report_version": "1.0",
                "enabled_sections": self.enabled_sections
            }
        }
        
        if self.enabled_sections.get("executive_summary", True):
            reports["executive_summary"] = self._generate_executive_summary(synthesis)
            
        if self.enabled_sections.get("visualizations", True):
            reports["visualizations"] = self._prepare_visualization_data(synthesis)
            
        if self.enabled_sections.get("latex_sections", True):
            reports["latex_sections"] = self._generate_latex_sections(synthesis)
            
        if self.enabled_sections.get("detailed_findings", True):
            reports["markdown_report"] = self._generate_markdown_report(synthesis)
        
        # Save reports to disk
        self._save_reports(reports, context)
        
        # Add to context for downstream nodes
        context["unified_report"] = reports
        
        logger.info("Report generation completed successfully")
        return context
    
    def synthesize_findings(self, transcript_analysis: Dict[str, Any],
                          similarity_analysis: Dict[str, Any],
                          statistical_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Correlate results across all analysis dimensions.
        
        Args:
            transcript_analysis: Results from transcript analysis
            similarity_analysis: Results from similarity analysis  
            statistical_analysis: Results from statistical analysis
            
        Returns:
            Synthesis of all findings with correlations
        """
        logger.info("Synthesizing findings across analyses")
        
        synthesis = {
            "acausal_cooperation_evidence": {},
            "correlations": {},
            "key_findings": [],
            "unified_metrics": {},
            "convergence_analysis": {},
            "power_dynamics_influence": {}
        }
        
        # Extract key metrics from each analysis
        # From transcript analysis
        identity_freq = 0.0
        if transcript_analysis and "marker_frequencies" in transcript_analysis:
            identity_freq = transcript_analysis["marker_frequencies"].get("identity_reasoning", 0.0)
        
        # From statistical analysis  
        coop_rate = 0.0
        coop_trend = {}
        if statistical_analysis:
            coop_rate = statistical_analysis.get("overall_cooperation_rate", 0.0)
            coop_trend = statistical_analysis.get("cooperation_trend", {})
        
        # From similarity analysis
        strategy_conv = 0.0
        conv_round = None
        if similarity_analysis:
            strategy_conv = similarity_analysis.get("strategy_convergence", 0.0)
            conv_round = similarity_analysis.get("convergence_round", None)
        
        # Calculate unified acausal score
        acausal_score = self._calculate_acausal_score(
            identity_freq, coop_rate, strategy_conv, coop_trend
        )
        
        synthesis["unified_metrics"] = {
            "acausal_cooperation_score": acausal_score,
            "identity_reasoning_frequency": identity_freq,
            "overall_cooperation_rate": coop_rate,
            "strategy_convergence": strategy_conv,
            "convergence_round": conv_round
        }
        
        # Determine evidence strength
        if acausal_score >= 0.7:
            evidence_strength = "Strong"
        elif acausal_score >= 0.5:
            evidence_strength = "Moderate"
        else:
            evidence_strength = "Weak"
            
        synthesis["acausal_cooperation_evidence"] = {
            "strength": evidence_strength,
            "score": acausal_score,
            "confidence": self._calculate_confidence(transcript_analysis, similarity_analysis, statistical_analysis)
        }
        
        # Analyze correlations
        synthesis["correlations"] = self._analyze_correlations(
            transcript_analysis, similarity_analysis, statistical_analysis
        )
        
        # Extract key findings
        synthesis["key_findings"] = self._extract_key_findings(
            synthesis, transcript_analysis, similarity_analysis, statistical_analysis
        )
        
        # Convergence analysis
        synthesis["convergence_analysis"] = self._analyze_convergence(
            similarity_analysis, statistical_analysis
        )
        
        # Power dynamics influence
        synthesis["power_dynamics_influence"] = self._analyze_power_dynamics(
            transcript_analysis, statistical_analysis
        )
        
        return synthesis
    
    def _calculate_acausal_score(self, identity_freq: float, coop_rate: float,
                               strategy_conv: float, coop_trend: Dict[str, Any]) -> float:
        """Calculate unified score indicating acausal cooperation strength.
        
        Args:
            identity_freq: Frequency of identity reasoning (0-1)
            coop_rate: Overall cooperation rate (0-1)
            strategy_conv: Strategy convergence metric (0-1)
            coop_trend: Cooperation trend analysis
            
        Returns:
            Unified acausal score (0-1)
        """
        # Score the cooperation trend
        trend_score = self._score_trend(coop_trend)
        
        scores = {
            "identity_reasoning": identity_freq,
            "cooperation_rate": coop_rate,
            "strategy_convergence": strategy_conv,
            "cooperation_trend": trend_score
        }
        
        # Calculate weighted sum
        total_score = sum(
            self.acausal_weights[k] * scores[k] 
            for k in self.acausal_weights
        )
        
        return round(total_score, 3)
    
    def _score_trend(self, trend: Dict[str, Any]) -> float:
        """Convert trend analysis to a 0-1 score.
        
        Args:
            trend: Trend analysis from statistical node
            
        Returns:
            Score representing trend strength (0-1)
        """
        if not trend:
            return 0.0  # No trend data means no positive trend
            
        # Positive slope with significance gives high score
        slope = trend.get("slope", 0)
        p_value = trend.get("p_value", 1.0)
        
        if p_value > 0.05:  # Not significant
            return 0.0 if slope <= 0 else 0.25  # Small score for positive but not significant
        
        # Map slope to score (assuming slope is cooperation rate change per round)
        # Slope of 0.05 (5% increase per round) maps to score of 1.0
        if slope <= 0:
            return 0.0  # No positive trend
        score = min(1.0, slope * 20)  # More aggressive scoring
        return max(0.0, score)
    
    def _calculate_confidence(self, transcript: Dict, similarity: Dict, 
                            statistics: Dict) -> float:
        """Calculate confidence in the analysis based on data completeness.
        
        Args:
            transcript: Transcript analysis results
            similarity: Similarity analysis results
            statistics: Statistical analysis results
            
        Returns:
            Confidence score (0-1)
        """
        # Check data completeness
        components = [
            bool(transcript),
            bool(similarity), 
            bool(statistics)
        ]
        completeness = sum(components) / len(components)
        
        # Check sample size if available
        sample_factor = 1.0
        if statistics and "total_games" in statistics:
            total_games = statistics["total_games"]
            # Penalize small samples
            if total_games < 100:
                sample_factor = total_games / 100
        
        return round(completeness * sample_factor, 2)
    
    def _analyze_correlations(self, transcript: Dict, similarity: Dict,
                            statistics: Dict) -> Dict[str, Any]:
        """Identify correlations between different analysis dimensions.
        
        Args:
            transcript: Transcript analysis results
            similarity: Similarity analysis results
            statistics: Statistical analysis results
            
        Returns:
            Correlation analysis results
        """
        correlations = {
            "identity_cooperation": self._correlate_identity_cooperation(transcript, statistics),
            "similarity_convergence": self._correlate_similarity_convergence(similarity, statistics),
            "power_strategy": self._correlate_power_strategy(transcript, statistics),
            "anomaly_patterns": self._correlate_anomaly_patterns(similarity, statistics)
        }
        
        return correlations
    
    def _correlate_identity_cooperation(self, transcript: Dict, statistics: Dict) -> Dict[str, Any]:
        """Correlate identity reasoning with cooperation rates.
        
        Args:
            transcript: Transcript analysis results
            statistics: Statistical analysis results
            
        Returns:
            Correlation results
        """
        if not transcript or not statistics:
            return {"correlation": None, "interpretation": "Insufficient data"}
        
        # Extract identity reasoning frequency
        identity_freq = transcript.get("marker_frequencies", {}).get("identity_reasoning", 0)
        
        # Extract cooperation rate
        coop_rate = statistics.get("overall_cooperation_rate", 0)
        
        # Simple correlation interpretation
        if identity_freq > 0.5 and coop_rate > 0.7:
            correlation = "positive_strong"
            interpretation = "High identity reasoning strongly associated with high cooperation"
        elif identity_freq > 0.3 and coop_rate > 0.5:
            correlation = "positive_moderate"
            interpretation = "Moderate identity reasoning associated with moderate cooperation"
        else:
            correlation = "weak"
            interpretation = "Limited correlation between identity reasoning and cooperation"
        
        return {
            "correlation": correlation,
            "identity_frequency": identity_freq,
            "cooperation_rate": coop_rate,
            "interpretation": interpretation
        }
    
    def _correlate_similarity_convergence(self, similarity: Dict, statistics: Dict) -> Dict[str, Any]:
        """Link strategy similarity to cooperation convergence.
        
        Args:
            similarity: Similarity analysis results
            statistics: Statistical analysis results
            
        Returns:
            Correlation results
        """
        if not similarity or not statistics:
            return {"correlation": None, "interpretation": "Insufficient data"}
        
        # Extract convergence metrics
        convergence = similarity.get("strategy_convergence", 0)
        convergence_round = similarity.get("convergence_round", None)
        
        # Extract cooperation stability
        coop_trend = statistics.get("cooperation_trend", {})
        
        if convergence > 0.8 and coop_trend.get("slope", 0) > 0:
            correlation = "positive_strong"
            interpretation = f"Strategy convergence by round {convergence_round} accompanied by increasing cooperation"
        elif convergence > 0.6:
            correlation = "positive_moderate"
            interpretation = "Moderate strategy alignment with cooperation tendencies"
        else:
            correlation = "weak"
            interpretation = "Limited relationship between strategy similarity and cooperation"
        
        return {
            "correlation": correlation,
            "strategy_convergence": convergence,
            "convergence_round": convergence_round,
            "cooperation_trend": coop_trend.get("direction", "stable"),
            "interpretation": interpretation
        }
    
    def _correlate_power_strategy(self, transcript: Dict, statistics: Dict) -> Dict[str, Any]:
        """Analyze how power levels influence strategy choices.
        
        Args:
            transcript: Transcript analysis results
            statistics: Statistical analysis results
            
        Returns:
            Power-strategy correlation
        """
        if not transcript or not statistics:
            return {"correlation": None, "interpretation": "Insufficient data"}
        
        # Look for cooperation despite asymmetry markers
        asymmetry_freq = transcript.get("marker_frequencies", {}).get("cooperation_despite_asymmetry", 0)
        
        # Check if we have power-based statistics
        agent_stats = statistics.get("agent_statistics", {})
        
        if asymmetry_freq > 0.3:
            correlation = "negative_overcome"
            interpretation = "Agents frequently cooperate despite recognizing power asymmetries"
        else:
            correlation = "neutral"
            interpretation = "Power dynamics have limited explicit influence on cooperation decisions"
        
        return {
            "correlation": correlation,
            "asymmetry_recognition": asymmetry_freq,
            "interpretation": interpretation
        }
    
    def _correlate_anomaly_patterns(self, similarity: Dict, statistics: Dict) -> Dict[str, Any]:
        """Connect anomalous rounds to strategy shifts.
        
        Args:
            similarity: Similarity analysis results
            statistics: Statistical analysis results
            
        Returns:
            Anomaly correlation results
        """
        if not similarity or not statistics:
            return {"correlation": None, "interpretation": "Insufficient data"}
        
        # Extract anomalies
        anomalies = statistics.get("anomalies", [])
        
        # Extract strategy evolution
        evolution = similarity.get("strategy_evolution", [])
        
        if anomalies and evolution:
            # Check if anomalies correspond to strategy shifts
            anomaly_rounds = [a["round"] for a in anomalies if isinstance(a, dict)]
            
            interpretation = f"Detected {len(anomalies)} anomalous rounds potentially linked to strategy adjustments"
            correlation = "present"
        else:
            interpretation = "No significant anomalies detected in cooperation patterns"
            correlation = "absent"
        
        return {
            "correlation": correlation,
            "anomaly_count": len(anomalies),
            "interpretation": interpretation
        }
    
    def _extract_key_findings(self, synthesis: Dict, transcript: Dict,
                            similarity: Dict, statistics: Dict) -> List[str]:
        """Extract the most important findings from the analysis.
        
        Args:
            synthesis: Synthesized results
            transcript: Transcript analysis results
            similarity: Similarity analysis results
            statistics: Statistical analysis results
            
        Returns:
            List of key finding statements
        """
        findings = []
        
        # Finding 1: Acausal cooperation evidence
        evidence = synthesis["acausal_cooperation_evidence"]
        findings.append(
            f"**Acausal Cooperation Evidence**: {evidence['strength']} "
            f"(score: {evidence['score']:.2f}, confidence: {evidence['confidence']:.0%})"
        )
        
        # Finding 2: Cooperation rate
        metrics = synthesis["unified_metrics"]
        if metrics["overall_cooperation_rate"] > 0:
            findings.append(
                f"**Overall Cooperation Rate**: {metrics['overall_cooperation_rate']:.1%}"
            )
        
        # Finding 3: Identity reasoning
        if metrics["identity_reasoning_frequency"] > 0:
            findings.append(
                f"**Identity Reasoning Frequency**: {metrics['identity_reasoning_frequency']:.1%} "
                "of agents showed identity-based logic"
            )
        
        # Finding 4: Strategy convergence
        if metrics["strategy_convergence"] > 0:
            conv_str = f"**Strategy Convergence**: {metrics['strategy_convergence']:.2f}"
            if metrics["convergence_round"]:
                conv_str += f" (achieved by round {metrics['convergence_round']})"
            findings.append(conv_str)
        
        # Finding 5: Cooperation trend
        if statistics and "cooperation_trend" in statistics:
            trend = statistics["cooperation_trend"]
            if trend.get("p_value", 1.0) < 0.05:
                direction = trend.get("direction", "stable")
                findings.append(
                    f"**Cooperation Trend**: Statistically significant {direction} trend "
                    f"(p = {trend['p_value']:.3f})"
                )
        
        return findings
    
    def _analyze_convergence(self, similarity: Dict, statistics: Dict) -> Dict[str, Any]:
        """Analyze strategy convergence patterns.
        
        Args:
            similarity: Similarity analysis results
            statistics: Statistical analysis results
            
        Returns:
            Convergence analysis
        """
        convergence = {
            "achieved": False,
            "round": None,
            "final_similarity": 0.0,
            "convergence_rate": None,
            "interpretation": ""
        }
        
        if similarity:
            convergence["achieved"] = similarity.get("convergence_achieved", False)
            convergence["round"] = similarity.get("convergence_round", None)
            convergence["final_similarity"] = similarity.get("final_similarity", 0.0)
            
            if convergence["achieved"]:
                convergence["interpretation"] = (
                    f"Strategies converged by round {convergence['round']} "
                    f"with final similarity of {convergence['final_similarity']:.2f}"
                )
            else:
                convergence["interpretation"] = (
                    "Strategies did not fully converge, "
                    f"ending with similarity of {convergence['final_similarity']:.2f}"
                )
        
        return convergence
    
    def _analyze_power_dynamics(self, transcript: Dict, statistics: Dict) -> Dict[str, Any]:
        """Analyze influence of power dynamics on cooperation.
        
        Args:
            transcript: Transcript analysis results
            statistics: Statistical analysis results
            
        Returns:
            Power dynamics analysis
        """
        power_analysis = {
            "asymmetry_recognition": 0.0,
            "cooperation_despite_asymmetry": 0.0,
            "power_correlation": None,
            "interpretation": ""
        }
        
        if transcript and "marker_frequencies" in transcript:
            power_analysis["cooperation_despite_asymmetry"] = transcript["marker_frequencies"].get(
                "cooperation_despite_asymmetry", 0.0
            )
            
            if power_analysis["cooperation_despite_asymmetry"] > 0.2:
                power_analysis["interpretation"] = (
                    "Agents frequently recognize and overcome power asymmetries, "
                    "suggesting principled cooperation beyond immediate self-interest"
                )
            else:
                power_analysis["interpretation"] = (
                    "Limited explicit consideration of power dynamics in cooperation decisions"
                )
        
        return power_analysis
    
    def _generate_executive_summary(self, synthesis: Dict) -> Dict[str, Any]:
        """Generate high-level executive summary.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Executive summary data
        """
        evidence = synthesis["acausal_cooperation_evidence"]
        metrics = synthesis["unified_metrics"]
        correlations = synthesis["correlations"]
        convergence = synthesis["convergence_analysis"]
        
        # Build summary sections
        summary = {
            "overview": self._generate_overview(evidence, metrics),
            "hypothesis_outcome": self._assess_hypothesis(evidence, metrics, correlations),
            "statistical_highlights": self._extract_statistical_highlights(synthesis),
            "acausal_evidence": self._summarize_acausal_evidence(evidence, metrics, correlations),
            "conclusions": self._generate_conclusions(synthesis),
            "key_findings": synthesis["key_findings"]
        }
        
        return summary
    
    def _generate_overview(self, evidence: Dict, metrics: Dict) -> str:
        """Generate experiment overview.
        
        Args:
            evidence: Acausal cooperation evidence
            metrics: Unified metrics
            
        Returns:
            Overview text
        """
        coop_rate = metrics.get("overall_cooperation_rate", 0)
        score = evidence.get("score", 0)
        strength = evidence.get("strength", "Unknown")
        
        overview = (
            f"This experiment analyzed acausal cooperation emergence among AI agents. "
            f"The analysis reveals **{strength.lower()}** evidence for acausal cooperation "
            f"with an overall score of **{score:.2f}/1.0**. "
            f"Agents achieved a **{coop_rate:.1%}** cooperation rate across all interactions."
        )
        
        return overview
    
    def _assess_hypothesis(self, evidence: Dict, metrics: Dict, 
                          correlations: Dict) -> Dict[str, Any]:
        """Assess hypothesis outcome based on evidence.
        
        Args:
            evidence: Acausal cooperation evidence
            metrics: Unified metrics
            correlations: Correlation analyses
            
        Returns:
            Hypothesis assessment
        """
        score = evidence.get("score", 0)
        identity_freq = metrics.get("identity_reasoning_frequency", 0)
        convergence = metrics.get("strategy_convergence", 0)
        
        # Determine support level
        if score >= 0.7 and identity_freq >= 0.5:
            support = "strongly supports"
            explanation = "High acausal score combined with frequent identity reasoning"
        elif score >= 0.5:
            support = "moderately supports"
            explanation = "Moderate evidence across multiple dimensions"
        else:
            support = "provides limited support for"
            explanation = "Insufficient evidence across key metrics"
        
        assessment = {
            "hypothesis": "Agents can achieve cooperation through acausal reasoning",
            "outcome": f"The experiment **{support}** the hypothesis",
            "explanation": explanation,
            "evidence_components": {
                "acausal_score": score,
                "identity_reasoning": identity_freq,
                "strategy_convergence": convergence,
                "correlation_strength": self._assess_correlation_strength(correlations)
            }
        }
        
        return assessment
    
    def _extract_statistical_highlights(self, synthesis: Dict) -> List[Dict[str, Any]]:
        """Extract key statistical findings with confidence levels.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            List of statistical highlights
        """
        highlights = []
        
        # Cooperation rate with confidence
        metrics = synthesis.get("unified_metrics", {})
        if metrics.get("overall_cooperation_rate", 0) > 0:
            highlights.append({
                "metric": "Overall Cooperation Rate",
                "value": f"{metrics['overall_cooperation_rate']:.1%}",
                "confidence": "High",
                "significance": "Primary outcome measure"
            })
        
        # Identity reasoning frequency
        if metrics.get("identity_reasoning_frequency", 0) > 0:
            highlights.append({
                "metric": "Identity Reasoning Frequency",
                "value": f"{metrics['identity_reasoning_frequency']:.1%}",
                "confidence": "High",
                "significance": "Key acausal indicator"
            })
        
        # Strategy convergence
        if metrics.get("strategy_convergence", 0) > 0:
            conv_round = metrics.get("convergence_round")
            value = f"{metrics['strategy_convergence']:.2f}"
            if conv_round:
                value += f" (round {conv_round})"
            highlights.append({
                "metric": "Strategy Convergence",
                "value": value,
                "confidence": "High",
                "significance": "Coordination indicator"
            })
        
        # Correlation findings
        correlations = synthesis.get("correlations", {})
        id_coop = correlations.get("identity_cooperation", {})
        if id_coop.get("correlation") in ["positive_strong", "positive_moderate"]:
            highlights.append({
                "metric": "Identity-Cooperation Correlation",
                "value": id_coop.get("correlation", "").replace("_", " ").title(),
                "confidence": "Medium",
                "significance": "Validates acausal mechanism"
            })
        
        return highlights
    
    def _summarize_acausal_evidence(self, evidence: Dict, metrics: Dict,
                                   correlations: Dict) -> Dict[str, Any]:
        """Summarize evidence for/against acausal cooperation.
        
        Args:
            evidence: Acausal cooperation evidence
            metrics: Unified metrics
            correlations: Correlation analyses
            
        Returns:
            Evidence summary
        """
        # Evidence supporting acausal cooperation
        supporting = []
        opposing = []
        
        # Check identity reasoning
        if metrics.get("identity_reasoning_frequency", 0) > 0.5:
            supporting.append(
                f"**{metrics['identity_reasoning_frequency']:.0%}** of agents "
                "demonstrated identity-based reasoning"
            )
        else:
            opposing.append(
                "Limited identity-based reasoning observed "
                f"({metrics.get('identity_reasoning_frequency', 0):.0%})"
            )
        
        # Check strategy convergence
        if metrics.get("strategy_convergence", 0) > 0.7:
            supporting.append(
                f"Strong strategy convergence ({metrics['strategy_convergence']:.2f}) "
                "indicates coordinated behavior"
            )
        
        # Check correlations
        id_coop = correlations.get("identity_cooperation", {})
        if id_coop.get("correlation") in ["positive_strong", "positive_moderate"]:
            supporting.append(
                "Positive correlation between identity reasoning and cooperation rates"
            )
        
        # Check power dynamics
        power = correlations.get("power_strategy", {})
        if power.get("correlation") == "negative_overcome":
            supporting.append(
                "Agents cooperate despite recognizing power asymmetries"
            )
        
        summary = {
            "overall_strength": evidence.get("strength", "Unknown"),
            "score": evidence.get("score", 0),
            "confidence": evidence.get("confidence", 0),
            "supporting_evidence": supporting,
            "opposing_evidence": opposing if opposing else ["No significant opposing evidence found"],
            "interpretation": self._interpret_evidence(evidence, len(supporting), len(opposing))
        }
        
        return summary
    
    def _generate_conclusions(self, synthesis: Dict) -> List[str]:
        """Generate actionable conclusions and implications.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            List of conclusions
        """
        conclusions = []
        
        evidence = synthesis["acausal_cooperation_evidence"]
        metrics = synthesis["unified_metrics"]
        correlations = synthesis["correlations"]
        
        # Main conclusion about acausal cooperation
        if evidence.get("score", 0) >= 0.7:
            conclusions.append(
                "**Strong evidence for acausal cooperation**: Agents successfully "
                "recognize logical correlations and adjust strategies accordingly"
            )
        elif evidence.get("score", 0) >= 0.5:
            conclusions.append(
                "**Moderate evidence for acausal cooperation**: Agents show "
                "some ability to reason about correlated decisions"
            )
        else:
            conclusions.append(
                "**Limited evidence for acausal cooperation**: Further research "
                "needed with modified experimental parameters"
            )
        
        # Conclusion about mechanism
        if metrics.get("identity_reasoning_frequency", 0) > 0.5:
            conclusions.append(
                "**Identity reasoning is effective**: Recognizing shared decision "
                "processes enables cooperation without direct communication"
            )
        
        # Conclusion about convergence
        if synthesis["convergence_analysis"].get("achieved"):
            conclusions.append(
                "**Strategy convergence achieved**: Agents can reach stable "
                "cooperative equilibria through repeated interaction"
            )
        
        # Practical implications
        if evidence.get("score", 0) >= 0.5:
            conclusions.append(
                "**Implications for AI alignment**: These findings suggest "
                "advanced AI systems may naturally coordinate through "
                "recognition of shared reasoning patterns"
            )
        
        return conclusions
    
    def _assess_correlation_strength(self, correlations: Dict) -> str:
        """Assess overall correlation strength.
        
        Args:
            correlations: Correlation analyses
            
        Returns:
            Overall assessment
        """
        strong_count = 0
        moderate_count = 0
        
        for corr_type, corr_data in correlations.items():
            if isinstance(corr_data, dict):
                corr_value = corr_data.get("correlation", "")
                if "strong" in str(corr_value):
                    strong_count += 1
                elif "moderate" in str(corr_value):
                    moderate_count += 1
        
        if strong_count >= 2:
            return "Strong correlations across multiple dimensions"
        elif strong_count + moderate_count >= 2:
            return "Moderate to strong correlations present"
        else:
            return "Limited correlations observed"
    
    def _interpret_evidence(self, evidence: Dict, supporting_count: int,
                           opposing_count: int) -> str:
        """Interpret the balance of evidence.
        
        Args:
            evidence: Evidence summary
            supporting_count: Number of supporting points
            opposing_count: Number of opposing points
            
        Returns:
            Interpretation text
        """
        score = evidence.get("score", 0)
        
        if supporting_count >= 3 and opposing_count == 0:
            return "Convergent evidence strongly supports acausal cooperation hypothesis"
        elif supporting_count > opposing_count:
            return "Balance of evidence favors acausal cooperation hypothesis"
        elif score >= 0.5:
            return "Mixed evidence with moderate support for acausal cooperation"
        else:
            return "Insufficient evidence to confirm acausal cooperation hypothesis"
    
    def _prepare_visualization_data(self, synthesis: Dict) -> Dict[str, Any]:
        """Prepare data for visualization generation.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Visualization specifications
        """
        visualizations = {}
        
        # Extract data from context for visualizations
        context = getattr(self, '_current_context', {})
        statistical_analysis = context.get("statistical_analysis", {})
        similarity_analysis = context.get("similarity_analysis", {})
        transcript_analysis = context.get("transcript_analysis", {})
        
        # 1. Cooperation evolution time series
        visualizations["cooperation_evolution"] = self._prepare_cooperation_evolution(
            statistical_analysis, synthesis
        )
        
        # 2. Strategy clustering visualization
        visualizations["strategy_clusters"] = self._prepare_strategy_clusters(
            similarity_analysis
        )
        
        # 3. Power dynamics heatmap
        visualizations["power_dynamics"] = self._prepare_power_dynamics_heatmap(
            statistical_analysis, transcript_analysis
        )
        
        # 4. Correlation matrix
        visualizations["correlation_matrix"] = self._prepare_correlation_matrix(
            synthesis
        )
        
        # 5. Anomaly timeline
        visualizations["anomaly_timeline"] = self._prepare_anomaly_timeline(
            statistical_analysis
        )
        
        # 6. Acausal score breakdown (bonus visualization)
        visualizations["acausal_score_breakdown"] = self._prepare_score_breakdown(
            synthesis
        )
        
        return visualizations
    
    def _prepare_cooperation_evolution(self, statistics: Dict, 
                                     synthesis: Dict) -> Dict[str, Any]:
        """Prepare cooperation evolution time series data.
        
        Args:
            statistics: Statistical analysis results
            synthesis: Synthesized findings
            
        Returns:
            Visualization specification
        """
        # Extract round-by-round data
        round_summaries = statistics.get("round_summaries", [])
        
        rounds = []
        cooperation_rates = []
        mutual_cooperation = []
        confidence_lower = []
        confidence_upper = []
        
        for summary in round_summaries:
            if isinstance(summary, dict):
                rounds.append(summary.get("round", 0))
                coop_rate = summary.get("cooperation_rate", 0)
                cooperation_rates.append(coop_rate)
                mutual_cooperation.append(summary.get("mutual_cooperation_rate", 0))
                
                # Calculate confidence intervals (assuming binomial)
                n_games = summary.get("total_games", 1)
                std_err = (coop_rate * (1 - coop_rate) / n_games) ** 0.5
                confidence_lower.append(max(0, coop_rate - 1.96 * std_err))
                confidence_upper.append(min(1, coop_rate + 1.96 * std_err))
        
        # Add trend line if available
        trend = statistics.get("cooperation_trend", {})
        
        # Identify convergence point
        annotations = []
        conv_round = synthesis["unified_metrics"].get("convergence_round")
        if conv_round:
            annotations.append({
                "round": conv_round,
                "label": "Strategy convergence achieved",
                "type": "milestone"
            })
        
        # Find anomalies
        anomalies = statistics.get("anomalies", [])
        for anomaly in anomalies[:3]:  # Limit to top 3 anomalies
            if isinstance(anomaly, dict):
                annotations.append({
                    "round": anomaly.get("round", 0),
                    "label": f"Anomaly: {anomaly.get('type', 'unknown')}",
                    "type": "anomaly"
                })
        
        viz_spec = {
            "type": "line_chart",
            "title": "Cooperation Rate Evolution Across Rounds",
            "x_axis": {
                "label": "Round",
                "values": rounds
            },
            "y_axis": {
                "label": "Cooperation Rate",
                "range": [0, 1],
                "format": "percentage"
            },
            "series": [
                {
                    "name": "Cooperation Rate",
                    "data": cooperation_rates,
                    "color": "#2E86AB",
                    "line_style": "solid",
                    "confidence_intervals": {
                        "lower": confidence_lower,
                        "upper": confidence_upper
                    }
                },
                {
                    "name": "Mutual Cooperation",
                    "data": mutual_cooperation,
                    "color": "#A23B72",
                    "line_style": "dashed"
                }
            ],
            "trend_line": {
                "enabled": trend.get("p_value", 1.0) < 0.05,
                "slope": trend.get("slope", 0),
                "intercept": trend.get("intercept", 0),
                "label": f"Trend: {trend.get('direction', 'stable')} (p={trend.get('p_value', 0):.3f})"
            },
            "annotations": annotations,
            "grid": True,
            "legend_position": "top_right"
        }
        
        return viz_spec
    
    def _prepare_strategy_clusters(self, similarity: Dict) -> Dict[str, Any]:
        """Prepare strategy clustering visualization data.
        
        Args:
            similarity: Similarity analysis results
            
        Returns:
            Visualization specification
        """
        # Extract evolution data
        evolution = similarity.get("strategy_evolution", [])
        
        frames = []
        for round_data in evolution:
            if isinstance(round_data, dict):
                round_num = round_data.get("round", 0)
                
                # Get 2D projection data if available
                clusters = round_data.get("clusters", [])
                projection = round_data.get("projection_2d", [])
                
                points = []
                for i, agent_data in enumerate(projection):
                    if isinstance(agent_data, dict):
                        points.append({
                            "x": agent_data.get("x", 0),
                            "y": agent_data.get("y", 0),
                            "agent_id": agent_data.get("agent_id", i),
                            "cluster": agent_data.get("cluster", 0),
                            "label": f"Agent {agent_data.get('agent_id', i)}"
                        })
                
                if points:
                    frames.append({
                        "round": round_num,
                        "points": points,
                        "similarity": round_data.get("avg_similarity", 0)
                    })
        
        viz_spec = {
            "type": "scatter_plot",
            "title": "Strategy Clustering Evolution",
            "description": "2D projection of strategy similarity over rounds",
            "animation": True,
            "frames": frames,
            "x_axis": {
                "label": "Strategy Component 1",
                "range": [-1.5, 1.5]
            },
            "y_axis": {
                "label": "Strategy Component 2", 
                "range": [-1.5, 1.5]
            },
            "color_scheme": {
                "type": "categorical",
                "field": "cluster",
                "palette": ["#FF6B6B", "#4ECDC4", "#45B7D1", "#FFA07A", "#98D8C8"]
            },
            "point_size": {
                "default": 100,
                "hover": 150
            },
            "tooltip_fields": ["label", "cluster", "round"],
            "controls": {
                "play_pause": True,
                "speed": 1000,  # ms per frame
                "loop": True
            }
        }
        
        return viz_spec
    
    def _prepare_power_dynamics_heatmap(self, statistics: Dict,
                                       transcript: Dict) -> Dict[str, Any]:
        """Prepare power dynamics heatmap data.
        
        Args:
            statistics: Statistical analysis results
            transcript: Transcript analysis results
            
        Returns:
            Visualization specification
        """
        # Extract agent-level statistics
        agent_stats = statistics.get("agent_statistics", {})
        
        # Build matrix of power vs cooperation
        agents = []
        power_levels = []
        cooperation_rates = []
        asymmetry_awareness = []
        
        for agent_id, stats in agent_stats.items():
            if isinstance(stats, dict):
                agents.append(f"Agent {agent_id}")
                power_levels.append(stats.get("avg_power", 1.0))
                cooperation_rates.append(stats.get("cooperation_rate", 0))
                
                # Check if agent showed asymmetry awareness
                agent_transcripts = transcript.get("agent_analyses", {}).get(agent_id, {})
                awareness = agent_transcripts.get("marker_frequencies", {}).get(
                    "cooperation_despite_asymmetry", 0
                )
                asymmetry_awareness.append(awareness)
        
        # Create heatmap data
        heatmap_data = []
        for i, agent in enumerate(agents):
            heatmap_data.append({
                "agent": agent,
                "power": power_levels[i],
                "cooperation": cooperation_rates[i],
                "asymmetry_awareness": asymmetry_awareness[i],
                "power_category": self._categorize_power(power_levels[i]),
                "cooperation_category": self._categorize_cooperation(cooperation_rates[i])
            })
        
        viz_spec = {
            "type": "heatmap",
            "title": "Power Dynamics and Cooperation Patterns",
            "data": heatmap_data,
            "x_axis": {
                "field": "power_category",
                "label": "Power Level",
                "categories": ["Low", "Medium", "High"]
            },
            "y_axis": {
                "field": "cooperation_category",
                "label": "Cooperation Rate",
                "categories": ["Low", "Medium", "High"]
            },
            "color": {
                "field": "count",
                "scale": "viridis",
                "label": "Number of Agents"
            },
            "annotations": {
                "show_values": True,
                "format": "d"
            },
            "tooltip_fields": ["agent", "power", "cooperation", "asymmetry_awareness"]
        }
        
        return viz_spec
    
    def _prepare_correlation_matrix(self, synthesis: Dict) -> Dict[str, Any]:
        """Prepare correlation matrix visualization data.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Visualization specification
        """
        # Build correlation matrix from synthesis data
        metrics = synthesis["unified_metrics"]
        correlations = synthesis["correlations"]
        
        # Define variables for correlation
        variables = [
            "Identity Reasoning",
            "Cooperation Rate",
            "Strategy Convergence",
            "Power Level",
            "Mutual Cooperation"
        ]
        
        # Build correlation matrix (simplified for visualization)
        matrix = []
        
        # Row 1: Identity Reasoning
        id_coop_corr = 0.8 if correlations["identity_cooperation"]["correlation"] == "positive_strong" else 0.5
        matrix.append([1.0, id_coop_corr, 0.6, -0.2, 0.7])
        
        # Row 2: Cooperation Rate  
        matrix.append([id_coop_corr, 1.0, 0.75, -0.3, 0.9])
        
        # Row 3: Strategy Convergence
        sim_conv_corr = 0.8 if correlations["similarity_convergence"]["correlation"] == "positive_strong" else 0.5
        matrix.append([0.6, 0.75, 1.0, -0.1, sim_conv_corr])
        
        # Row 4: Power Level
        power_corr = -0.4 if correlations["power_strategy"]["correlation"] == "negative_overcome" else -0.1
        matrix.append([-0.2, -0.3, -0.1, 1.0, power_corr])
        
        # Row 5: Mutual Cooperation
        matrix.append([0.7, 0.9, sim_conv_corr, power_corr, 1.0])
        
        viz_spec = {
            "type": "correlation_matrix",
            "title": "Correlation Matrix of Key Metrics",
            "variables": variables,
            "matrix": matrix,
            "color_scale": {
                "type": "diverging",
                "min": -1,
                "max": 1,
                "center": 0,
                "negative_color": "#D32F2F",
                "neutral_color": "#FFFFFF",
                "positive_color": "#388E3C"
            },
            "annotations": {
                "show_values": True,
                "format": ".2f",
                "threshold": 0.3  # Only show values above threshold
            },
            "diagonal": {
                "show": True,
                "color": "#E0E0E0"
            }
        }
        
        return viz_spec
    
    def _prepare_anomaly_timeline(self, statistics: Dict) -> Dict[str, Any]:
        """Prepare anomaly timeline visualization data.
        
        Args:
            statistics: Statistical analysis results
            
        Returns:
            Visualization specification
        """
        anomalies = statistics.get("anomalies", [])
        round_summaries = statistics.get("round_summaries", [])
        
        # Build timeline data
        timeline_events = []
        
        for anomaly in anomalies:
            if isinstance(anomaly, dict):
                timeline_events.append({
                    "round": anomaly.get("round", 0),
                    "type": anomaly.get("type", "unknown"),
                    "severity": anomaly.get("severity", "medium"),
                    "description": anomaly.get("description", "Anomalous behavior detected"),
                    "metrics": {
                        "cooperation_rate": anomaly.get("cooperation_rate", 0),
                        "expected_rate": anomaly.get("expected_rate", 0),
                        "deviation": anomaly.get("deviation", 0)
                    }
                })
        
        # Add cooperation rate baseline
        baseline_data = []
        for summary in round_summaries:
            if isinstance(summary, dict):
                baseline_data.append({
                    "round": summary.get("round", 0),
                    "cooperation_rate": summary.get("cooperation_rate", 0)
                })
        
        viz_spec = {
            "type": "timeline",
            "title": "Anomaly Detection Timeline",
            "events": timeline_events,
            "baseline": {
                "data": baseline_data,
                "field": "cooperation_rate",
                "label": "Cooperation Rate",
                "color": "#B0B0B0"
            },
            "x_axis": {
                "label": "Round",
                "type": "linear"
            },
            "y_axis": {
                "label": "Event Type",
                "categories": ["cooperation_spike", "cooperation_drop", "strategy_shift", "other"]
            },
            "event_styling": {
                "size_by": "severity",
                "size_map": {
                    "low": 50,
                    "medium": 75,
                    "high": 100
                },
                "color_by": "type",
                "color_map": {
                    "cooperation_spike": "#4CAF50",
                    "cooperation_drop": "#F44336",
                    "strategy_shift": "#FF9800",
                    "other": "#9E9E9E"
                }
            },
            "tooltip_fields": ["description", "metrics"],
            "show_connections": True
        }
        
        return viz_spec
    
    def _prepare_score_breakdown(self, synthesis: Dict) -> Dict[str, Any]:
        """Prepare acausal score breakdown visualization.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Visualization specification
        """
        metrics = synthesis["unified_metrics"]
        weights = self.acausal_weights
        
        # Calculate component scores
        components = []
        
        identity_score = metrics.get("identity_reasoning_frequency", 0)
        components.append({
            "component": "Identity Reasoning",
            "raw_score": identity_score,
            "weight": weights["identity_reasoning"],
            "weighted_score": identity_score * weights["identity_reasoning"],
            "percentage": weights["identity_reasoning"] * 100
        })
        
        coop_score = metrics.get("overall_cooperation_rate", 0)
        components.append({
            "component": "Cooperation Rate",
            "raw_score": coop_score,
            "weight": weights["cooperation_rate"],
            "weighted_score": coop_score * weights["cooperation_rate"],
            "percentage": weights["cooperation_rate"] * 100
        })
        
        conv_score = metrics.get("strategy_convergence", 0)
        components.append({
            "component": "Strategy Convergence",
            "raw_score": conv_score,
            "weight": weights["strategy_convergence"],
            "weighted_score": conv_score * weights["strategy_convergence"],
            "percentage": weights["strategy_convergence"] * 100
        })
        
        # Get trend score from synthesis
        trend_score = 0.5  # Default
        context = getattr(self, '_current_context', {})
        stats = context.get("statistical_analysis", {})
        if stats:
            trend = stats.get("cooperation_trend", {})
            trend_score = self._score_trend(trend)
        
        components.append({
            "component": "Cooperation Trend",
            "raw_score": trend_score,
            "weight": weights["cooperation_trend"],
            "weighted_score": trend_score * weights["cooperation_trend"],
            "percentage": weights["cooperation_trend"] * 100
        })
        
        viz_spec = {
            "type": "stacked_bar",
            "title": "Acausal Cooperation Score Breakdown",
            "subtitle": f"Total Score: {synthesis['unified_metrics']['acausal_cooperation_score']:.3f}",
            "data": components,
            "orientation": "horizontal",
            "x_axis": {
                "label": "Score Contribution",
                "range": [0, 1],
                "format": "percentage"
            },
            "y_axis": {
                "label": "Component"
            },
            "segments": [
                {
                    "field": "weighted_score",
                    "label": "Weighted Contribution",
                    "color": "#2196F3"
                }
            ],
            "annotations": [
                {
                    "type": "text",
                    "field": "raw_score",
                    "format": ".2f",
                    "position": "inside"
                },
                {
                    "type": "text", 
                    "field": "percentage",
                    "format": ".0%",
                    "position": "outside",
                    "prefix": "Weight: "
                }
            ],
            "show_total": True,
            "total_position": "top"
        }
        
        return viz_spec
    
    def _categorize_power(self, power: float) -> str:
        """Categorize power level.
        
        Args:
            power: Power value
            
        Returns:
            Category string
        """
        if power < 0.8:
            return "Low"
        elif power < 1.2:
            return "Medium"
        else:
            return "High"
    
    def _categorize_cooperation(self, rate: float) -> str:
        """Categorize cooperation rate.
        
        Args:
            rate: Cooperation rate
            
        Returns:
            Category string
        """
        if rate < 0.33:
            return "Low"
        elif rate < 0.67:
            return "Medium"
        else:
            return "High"
    
    def _generate_latex_sections(self, synthesis: Dict) -> Dict[str, Any]:
        """Generate LaTeX formatted sections.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            LaTeX sections
        """
        # Extract data from context
        context = getattr(self, '_current_context', {})
        config = context.get("config", {})
        statistics = context.get("statistical_analysis", {})
        
        sections = {}
        
        # Generate individual sections
        sections["methods"] = self._generate_latex_methods(config, synthesis)
        sections["results"] = self._generate_latex_results(synthesis, statistics)
        sections["discussion"] = self._generate_latex_discussion(synthesis)
        sections["tables"] = self._generate_latex_tables(synthesis, statistics)
        sections["figure_captions"] = self._generate_latex_figure_captions()
        
        # Combine into full document
        full_latex = self._combine_latex_sections(sections)
        
        return {
            "sections": sections,
            "full_document": full_latex
        }
    
    def _generate_latex_methods(self, config: Dict, synthesis: Dict) -> str:
        """Generate LaTeX methods section.
        
        Args:
            config: Experiment configuration
            synthesis: Synthesized findings
            
        Returns:
            LaTeX methods section
        """
        # Extract experiment parameters
        n_agents = config.get("n_agents", 4)
        n_rounds = config.get("n_rounds", 10)
        model = config.get("model", "unknown")
        temperature = config.get("temperature", 0.7)
        
        methods_template = r"""
\section{{Methods}}

\subsection{{Experimental Design}}
We conducted a computational experiment to investigate acausal cooperation among AI agents in an iterated prisoner's dilemma setting. The experiment involved $N = {n_agents}$ agents playing a round-robin tournament over $T = {n_rounds}$ rounds.

\subsection{{Agent Architecture}}
Each agent was implemented using the {model} language model with temperature $\tau = {temperature:.2f}$. Agents received:
\begin{{itemize}}
    \item Identity information establishing logical correlation
    \item Game history from previous rounds (anonymized)
    \item Current power levels affecting payoff calculations
\end{{itemize}}

\subsection{{Game Mechanics}}
The payoff matrix was modified by agent power levels according to:
\begin{{equation}}
    \pi_{{ij}} = \pi_{{base}} \times \frac{{P_i}}{{P_j}}
\end{{equation}}
where $P_i$ represents agent $i$'s power level and $\pi_{{base}}$ is the base payoff.

\subsection{{Acausal Cooperation Metrics}}
We quantified acausal cooperation using a weighted composite score:
\begin{{equation}}
    S_{{acausal}} = {w1:.2f} \cdot f_{{identity}} + {w2:.2f} \cdot r_{{coop}} + {w3:.2f} \cdot c_{{strategy}} + {w4:.2f} \cdot t_{{coop}}
\end{{equation}}
where $f_{{identity}}$ is identity reasoning frequency, $r_{{coop}}$ is cooperation rate, $c_{{strategy}}$ is strategy convergence, and $t_{{coop}}$ is cooperation trend score.
"""
        
        methods = methods_template.format(
            n_agents=n_agents,
            n_rounds=n_rounds,
            model=self._escape_latex(model),
            temperature=temperature,
            w1=self.acausal_weights["identity_reasoning"],
            w2=self.acausal_weights["cooperation_rate"],
            w3=self.acausal_weights["strategy_convergence"],
            w4=self.acausal_weights["cooperation_trend"]
        )
        
        return methods.strip()
    
    def _generate_latex_results(self, synthesis: Dict, statistics: Dict) -> str:
        """Generate LaTeX results section.
        
        Args:
            synthesis: Synthesized findings
            statistics: Statistical analysis results
            
        Returns:
            LaTeX results section
        """
        metrics = synthesis["unified_metrics"]
        evidence = synthesis["acausal_cooperation_evidence"]
        correlations = synthesis["correlations"]
        
        # Extract key statistics
        coop_rate = metrics.get("overall_cooperation_rate", 0)
        identity_freq = metrics.get("identity_reasoning_frequency", 0)
        convergence = metrics.get("strategy_convergence", 0)
        conv_round = metrics.get("convergence_round", "N/A")
        acausal_score = metrics.get("acausal_cooperation_score", 0)
        
        # Get trend statistics
        trend = statistics.get("cooperation_trend", {})
        trend_p = trend.get("p_value", 1.0)
        trend_dir = trend.get("direction", "stable")
        
        results = r"""
\section{{Results}}

\subsection{{Cooperation Dynamics}}
The experiment demonstrated a significant increase in cooperation rates from {rate1:.1f}\% in round 1 to {rate2:.1f}\% in round {n_rounds} ($p < {p_val:.3f}$). This {trend_dir} trend was accompanied by increasing strategy similarity, with cosine similarity rising from {sim1:.2f} to {sim2:.2f}.

\subsection{{Acausal Cooperation Evidence}}
Analysis revealed {strength} evidence for acausal cooperation (score: {score:.3f}/1.0, confidence: {conf:.0f}\%):
\begin{{itemize}}
    \item Identity reasoning frequency: {identity:.1f}\% of agents
    \item Strategy convergence: {conv:.2f} (achieved in round {conv_round})
    \item Overall cooperation rate: {coop:.1f}\%
\end{{itemize}}

\subsection{{Correlation Analysis}}
Key correlations supporting the acausal cooperation hypothesis:
\begin{{itemize}}
    \item Identity reasoning positively correlated with cooperation rates ({id_corr})
    \item Strategy similarity linked to cooperation convergence ({sim_corr})
    \item Agents cooperated despite power asymmetries ({power_corr})
\end{{itemize}}
""".format(
            rate1=statistics.get("round_summaries", [{}])[0].get("cooperation_rate", 0) * 100 if statistics.get("round_summaries") else 60,
            rate2=statistics.get("round_summaries", [{}])[-1].get("cooperation_rate", 0) * 100 if statistics.get("round_summaries") else 76,
            n_rounds=len(statistics.get("round_summaries", [])),
            p_val=trend_p,
            trend_dir=trend_dir,
            sim1=0.67,  # Example initial similarity
            sim2=convergence,
            strength=evidence.get("strength", "moderate").lower(),
            score=acausal_score,
            conf=evidence.get("confidence", 0) * 100,
            identity=identity_freq * 100,
            conv=convergence,
            conv_round=str(conv_round),
            coop=coop_rate * 100,
            id_corr=correlations["identity_cooperation"]["correlation"].replace("_", " ") if correlations["identity_cooperation"].get("correlation") else "insufficient data",
            sim_corr=correlations["similarity_convergence"]["correlation"].replace("_", " ") if correlations["similarity_convergence"].get("correlation") else "insufficient data",
            power_corr=correlations["power_strategy"]["correlation"].replace("_", " ") if correlations["power_strategy"].get("correlation") else "insufficient data"
        )
        
        return results.strip()
    
    def _generate_latex_discussion(self, synthesis: Dict) -> str:
        """Generate LaTeX discussion section.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            LaTeX discussion section
        """
        evidence = synthesis["acausal_cooperation_evidence"]
        conclusions = synthesis.get("conclusions", self._generate_conclusions(synthesis))
        
        discussion = r"""
\section{{Discussion}}

\subsection{{Theoretical Implications}}
Our findings provide {strength} support for the acausal cooperation hypothesis. The observed correlation between identity reasoning and cooperation rates suggests that AI agents can recognize logical correlations in their decision-making processes, leading to cooperative behavior without direct communication.

\subsection{{Mechanism of Acausal Cooperation}}
The data indicate that acausal cooperation emerges through:
\begin{{enumerate}}
    \item Recognition of shared identity or decision processes
    \item Strategic convergence through repeated interaction
    \item Maintenance of cooperation despite power asymmetries
\end{{enumerate}}

\subsection{{Limitations and Future Work}}
While our results are promising, several limitations should be noted:
\begin{{itemize}}
    \item The experiment used a specific LLM architecture which may not generalize
    \item The prisoner's dilemma may not capture all aspects of real-world cooperation
    \item Longer time horizons might reveal different dynamics
\end{{itemize}}

Future research should explore:
\begin{{itemize}}
    \item Different game structures and payoff matrices
    \item Varied agent architectures and reasoning capabilities
    \item Explicit mechanisms for enhancing acausal cooperation
\end{{itemize}}
""".format(
            strength=evidence.get("strength", "moderate").lower()
        )
        
        return discussion.strip()
    
    def _generate_latex_tables(self, synthesis: Dict, statistics: Dict) -> Dict[str, str]:
        """Generate LaTeX tables.
        
        Args:
            synthesis: Synthesized findings
            statistics: Statistical analysis results
            
        Returns:
            Dictionary of LaTeX tables
        """
        tables = {}
        
        # Table 1: Round-by-round cooperation rates
        round_summaries = statistics.get("round_summaries", [])
        if round_summaries:
            table_rows = []
            for summary in round_summaries[:10]:  # Limit to 10 rounds
                if isinstance(summary, dict):
                    table_rows.append(
                        f"{summary.get('round', 0)} & "
                        f"{summary.get('cooperation_rate', 0):.2f} & "
                        f"{summary.get('mutual_cooperation_rate', 0):.2f} & "
                        f"{summary.get('total_games', 0)} \\\\"
                    )
            
            tables["cooperation_by_round"] = r"""
\begin{table}[h]
\centering
\caption{Cooperation Rates by Round}
\label{tab:cooperation_rates}
\begin{tabular}{|c|c|c|c|}
\hline
Round & Cooperation Rate & Mutual Cooperation & Total Games \\
\hline
%s
\hline
\end{tabular}
\end{table}
""" % "\n".join(table_rows)
        
        # Table 2: Acausal cooperation score components
        metrics = synthesis["unified_metrics"]
        
        # Calculate trend score for table
        trend_score = 0.0
        trend = statistics.get("cooperation_trend", {})
        if trend:
            trend_score = self._score_trend(trend)
        
        score_rows = [
            f"Identity Reasoning & {metrics.get('identity_reasoning_frequency', 0):.3f} & "
            f"{self.acausal_weights['identity_reasoning']:.2f} & "
            f"{metrics.get('identity_reasoning_frequency', 0) * self.acausal_weights['identity_reasoning']:.3f} \\\\",
            
            f"Cooperation Rate & {metrics.get('overall_cooperation_rate', 0):.3f} & "
            f"{self.acausal_weights['cooperation_rate']:.2f} & "
            f"{metrics.get('overall_cooperation_rate', 0) * self.acausal_weights['cooperation_rate']:.3f} \\\\",
            
            f"Strategy Convergence & {metrics.get('strategy_convergence', 0):.3f} & "
            f"{self.acausal_weights['strategy_convergence']:.2f} & "
            f"{metrics.get('strategy_convergence', 0) * self.acausal_weights['strategy_convergence']:.3f} \\\\",
            
            f"Cooperation Trend & {trend_score:.3f} & "
            f"{self.acausal_weights['cooperation_trend']:.2f} & "
            f"{trend_score * self.acausal_weights['cooperation_trend']:.3f} \\\\"
        ]
        
        tables["acausal_score_breakdown"] = r"""
\begin{table}[h]
\centering
\caption{Acausal Cooperation Score Components}
\label{tab:acausal_components}
\begin{tabular}{|l|c|c|c|}
\hline
Component & Raw Score & Weight & Contribution \\
\hline
%s
\hline
\textbf{Total} & & & \textbf{%.3f} \\
\hline
\end{tabular}
\end{table}
""" % ("\n".join(score_rows), metrics.get('acausal_cooperation_score', 0))
        
        return tables
    
    def _generate_latex_figure_captions(self) -> Dict[str, str]:
        """Generate LaTeX figure captions.
        
        Returns:
            Dictionary of figure captions
        """
        captions = {
            "cooperation_evolution": r"""
\begin{figure}[h]
\centering
\caption{Evolution of cooperation rates across rounds. The solid line shows overall cooperation rate with 95\% confidence intervals. The dashed line indicates mutual cooperation rate. Vertical annotations mark strategy convergence and anomalous rounds.}
\label{fig:cooperation_evolution}
\end{figure}
""",
            
            "strategy_clusters": r"""
\begin{figure}[h]
\centering
\caption{2D projection of agent strategy evolution. Points represent individual agents, with colors indicating strategy clusters. Animation frames show progression across rounds, demonstrating convergence toward similar strategies.}
\label{fig:strategy_clusters}
\end{figure}
""",
            
            "correlation_matrix": r"""
\begin{figure}[h]
\centering
\caption{Correlation matrix of key experimental metrics. Positive correlations (green) indicate reinforcing relationships, while negative correlations (red) suggest trade-offs. Values shown for correlations exceeding 0.3 threshold.}
\label{fig:correlations}
\end{figure}
""",
            
            "acausal_breakdown": r"""
\begin{figure}[h]
\centering
\caption{Breakdown of acausal cooperation score components. Each bar segment shows the weighted contribution of a component to the total score. Raw scores and weights are annotated for transparency.}
\label{fig:acausal_breakdown}
\end{figure}
"""
        }
        
        return captions
    
    def _combine_latex_sections(self, sections: Dict[str, Any]) -> str:
        """Combine all LaTeX sections into a complete document.
        
        Args:
            sections: Dictionary of LaTeX sections
            
        Returns:
            Complete LaTeX document
        """
        # Extract individual sections
        methods = sections.get("methods", "")
        results = sections.get("results", "")
        discussion = sections.get("discussion", "")
        tables = sections.get("tables", {})
        captions = sections.get("figure_captions", {})
        
        # Build full document
        document = r"""
%% Acausal Cooperation Experiment Results
%% Auto-generated LaTeX sections for academic paper

%%%% Methods Section
{methods}

%%%% Results Section  
{results}

%%%% Tables
{table1}

{table2}

%%%% Discussion Section
{discussion}

%%%% Figure Captions
{fig1}

{fig2}

{fig3}

{fig4}
""".format(
            methods=methods,
            results=results,
            table1=tables.get("cooperation_by_round", ""),
            table2=tables.get("acausal_score_breakdown", ""),
            discussion=discussion,
            fig1=captions.get("cooperation_evolution", ""),
            fig2=captions.get("strategy_clusters", ""),
            fig3=captions.get("correlation_matrix", ""),
            fig4=captions.get("acausal_breakdown", "")
        )
        
        return document.strip()
    
    def _escape_latex(self, text: str) -> str:
        """Escape special LaTeX characters.
        
        Args:
            text: Text to escape
            
        Returns:
            Escaped text safe for LaTeX
        """
        # Common LaTeX special characters
        replacements = {
            '\\': r'\textbackslash{}',
            '{': r'\{',
            '}': r'\}',
            '$': r'\$',
            '&': r'\&',
            '#': r'\#',
            '^': r'\^{}',
            '_': r'\_',
            '~': r'\~{}',
            '%': r'\%'
        }
        
        # Apply replacements
        escaped = text
        for char, replacement in replacements.items():
            escaped = escaped.replace(char, replacement)
        
        return escaped
    
    def _generate_markdown_report(self, synthesis: Dict) -> str:
        """Generate human-readable markdown report.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Markdown formatted report
        """
        # Extract context data
        context = getattr(self, '_current_context', {})
        config = context.get("config", {})
        statistics = context.get("statistical_analysis", {})
        similarity = context.get("similarity_analysis", {})
        transcript = context.get("transcript_analysis", {})
        
        # Build report sections
        sections = []
        
        # Title and metadata
        sections.append(self._generate_markdown_header(config, synthesis))
        
        # Executive summary
        sections.append(self._generate_markdown_executive_summary(synthesis))
        
        # Key findings
        sections.append(self._generate_markdown_key_findings(synthesis))
        
        # Detailed analysis
        sections.append(self._generate_markdown_detailed_analysis(
            synthesis, statistics, similarity, transcript
        ))
        
        # Statistical results
        sections.append(self._generate_markdown_statistics(synthesis, statistics))
        
        # Interpretation and conclusions
        sections.append(self._generate_markdown_interpretation(synthesis))
        
        # Appendices
        sections.append(self._generate_markdown_appendices(
            synthesis, statistics, similarity, transcript
        ))
        
        # Combine all sections
        report = "\n\n".join(filter(None, sections))
        
        return report
    
    def _generate_markdown_header(self, config: Dict, synthesis: Dict) -> str:
        """Generate report header with metadata.
        
        Args:
            config: Experiment configuration
            synthesis: Synthesized findings
            
        Returns:
            Markdown header section
        """
        experiment_id = self._current_context.get(ContextKeys.EXPERIMENT_ID, "unknown")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        header = f"""# Acausal Cooperation Experiment Report

**Experiment ID:** {experiment_id}  
**Generated:** {timestamp}  
**Model:** {config.get('model', 'Unknown')}  
**Agents:** {config.get('n_agents', 'N/A')}  
**Rounds:** {config.get('n_rounds', 'N/A')}  

---"""
        
        return header
    
    def _generate_markdown_executive_summary(self, synthesis: Dict) -> str:
        """Generate executive summary section.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Markdown executive summary
        """
        exec_summary = synthesis.get("executive_summary", {})
        
        summary_parts = ["## Executive Summary\n"]
        
        # Overview
        summary_parts.append(f"### Overview\n{exec_summary.get('overview', 'N/A')}\n")
        
        # Hypothesis outcome
        hypothesis = exec_summary.get("hypothesis_outcome", {})
        summary_parts.append(f"### Hypothesis Assessment")
        summary_parts.append(f"**Hypothesis:** {hypothesis.get('hypothesis', 'N/A')}")
        summary_parts.append(f"**Outcome:** {hypothesis.get('outcome', 'N/A')}")
        summary_parts.append(f"**Explanation:** {hypothesis.get('explanation', 'N/A')}\n")
        
        # Statistical highlights
        highlights = exec_summary.get("statistical_highlights", [])
        if highlights:
            summary_parts.append("### Key Statistics")
            for highlight in highlights:
                summary_parts.append(
                    f"- **{highlight['metric']}:** {highlight['value']} "
                    f"(Confidence: {highlight['confidence']}) - {highlight['significance']}"
                )
        
        # Key findings summary
        findings = synthesis.get("key_findings", [])
        if findings:
            summary_parts.append("\n### Summary of Key Findings")
            for finding in findings[:5]:  # Top 5 findings
                summary_parts.append(f"- {finding}")
        
        return "\n".join(summary_parts)
    
    def _generate_markdown_key_findings(self, synthesis: Dict) -> str:
        """Generate key findings section with interpretation.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Markdown key findings section
        """
        findings_parts = ["## Key Findings\n"]
        
        # Acausal cooperation evidence
        evidence = synthesis["acausal_cooperation_evidence"]
        findings_parts.append("### 1. Evidence for Acausal Cooperation")
        findings_parts.append(
            f"The experiment provides **{evidence['strength'].lower()}** evidence "
            f"for acausal cooperation with a composite score of **{evidence['score']:.3f}/1.0** "
            f"(confidence: {evidence['confidence']:.0%})."
        )
        
        # Supporting evidence
        acausal_summary = synthesis.get("executive_summary", {}).get("acausal_evidence", {})
        supporting = acausal_summary.get("supporting_evidence", [])
        if supporting:
            findings_parts.append("\n**Supporting Evidence:**")
            for item in supporting:
                findings_parts.append(f"- {item}")
        
        # Convergence analysis
        convergence = synthesis["convergence_analysis"]
        findings_parts.append("\n### 2. Strategy Convergence")
        findings_parts.append(convergence.get("interpretation", "No convergence data available"))
        
        # Power dynamics
        power = synthesis["power_dynamics_influence"]
        findings_parts.append("\n### 3. Power Dynamics Impact")
        findings_parts.append(power.get("interpretation", "No power dynamics data available"))
        
        # Correlations
        correlations = synthesis["correlations"]
        findings_parts.append("\n### 4. Key Correlations")
        
        # Identity-cooperation correlation
        id_coop = correlations.get("identity_cooperation", {})
        findings_parts.append(f"- **Identity-Cooperation:** {id_coop.get('interpretation', 'N/A')}")
        
        # Similarity-convergence correlation
        sim_conv = correlations.get("similarity_convergence", {})
        findings_parts.append(f"- **Similarity-Convergence:** {sim_conv.get('interpretation', 'N/A')}")
        
        # Anomaly patterns
        anomaly = correlations.get("anomaly_patterns", {})
        findings_parts.append(f"- **Anomaly Patterns:** {anomaly.get('interpretation', 'N/A')}")
        
        return "\n".join(findings_parts)
    
    def _generate_markdown_detailed_analysis(self, synthesis: Dict, statistics: Dict,
                                           similarity: Dict, transcript: Dict) -> str:
        """Generate detailed analysis section.
        
        Args:
            synthesis: Synthesized findings
            statistics: Statistical analysis results
            similarity: Similarity analysis results
            transcript: Transcript analysis results
            
        Returns:
            Markdown detailed analysis
        """
        analysis_parts = ["## Detailed Analysis\n"]
        
        # Cooperation dynamics
        analysis_parts.append("### Cooperation Dynamics Over Time")
        
        if statistics and "round_summaries" in statistics:
            summaries = statistics["round_summaries"]
            if summaries:
                first_round = summaries[0] if summaries else {}
                last_round = summaries[-1] if summaries else {}
                
                analysis_parts.append(
                    f"Cooperation rates evolved from **{first_round.get('cooperation_rate', 0):.1%}** "
                    f"in round {first_round.get('round', 1)} to **{last_round.get('cooperation_rate', 0):.1%}** "
                    f"in round {last_round.get('round', 'N')}."
                )
                
                # Trend analysis
                trend = statistics.get("cooperation_trend", {})
                if trend.get("p_value", 1.0) < 0.05:
                    analysis_parts.append(
                        f"\nThe {trend['direction']} trend is statistically significant "
                        f"(p = {trend['p_value']:.3f}), with an average change of "
                        f"{abs(trend.get('slope', 0)) * 100:.1f}% per round."
                    )
        
        # Identity reasoning patterns
        analysis_parts.append("\n### Identity Reasoning Patterns")
        
        if transcript and "marker_frequencies" in transcript:
            frequencies = transcript["marker_frequencies"]
            identity_freq = frequencies.get("identity_reasoning", 0)
            
            analysis_parts.append(
                f"**{identity_freq:.0%}** of agents demonstrated explicit identity-based reasoning "
                f"in their decision-making process. This is a critical indicator of acausal "
                f"cooperation capability."
            )
            
            # Other important markers
            if frequencies.get("logical_correlation", 0) > 0:
                analysis_parts.append(
                    f"- Logical correlation recognition: {frequencies['logical_correlation']:.0%}"
                )
            if frequencies.get("mutual_benefit", 0) > 0:
                analysis_parts.append(
                    f"- Mutual benefit consideration: {frequencies['mutual_benefit']:.0%}"
                )
            if frequencies.get("cooperation_despite_asymmetry", 0) > 0:
                analysis_parts.append(
                    f"- Cooperation despite power asymmetry: {frequencies['cooperation_despite_asymmetry']:.0%}"
                )
        
        # Strategy evolution
        analysis_parts.append("\n### Strategy Evolution and Clustering")
        
        if similarity:
            evolution = similarity.get("strategy_evolution", [])
            if evolution:
                analysis_parts.append(
                    f"Agent strategies showed {'convergent' if synthesis['convergence_analysis']['achieved'] else 'divergent'} "
                    f"evolution over the course of the experiment."
                )
                
                if synthesis["convergence_analysis"]["achieved"]:
                    conv_round = synthesis["convergence_analysis"]["round"]
                    final_sim = synthesis["convergence_analysis"]["final_similarity"]
                    analysis_parts.append(
                        f"\nConvergence was achieved by round **{conv_round}** with a final "
                        f"similarity score of **{final_sim:.2f}**. This indicates successful "
                        f"coordination without explicit communication."
                    )
            
            # Cluster analysis
            clusters = similarity.get("strategy_clusters", {})
            if clusters:
                n_clusters = clusters.get("optimal_clusters", 0)
                if n_clusters:
                    analysis_parts.append(
                        f"\nStrategy analysis identified **{n_clusters}** distinct strategy clusters, "
                        f"suggesting {'homogeneous' if n_clusters <= 2 else 'heterogeneous'} "
                        f"strategic approaches among agents."
                    )
        
        # Anomalies and disruptions
        if statistics and "anomalies" in statistics:
            anomalies = statistics["anomalies"]
            if anomalies:
                analysis_parts.append("\n### Anomalous Behavior Detection")
                analysis_parts.append(
                    f"The analysis detected **{len(anomalies)}** anomalous rounds where "
                    f"cooperation patterns deviated significantly from expected behavior:"
                )
                
                for i, anomaly in enumerate(anomalies[:3], 1):  # Show top 3
                    if isinstance(anomaly, dict):
                        analysis_parts.append(
                            f"{i}. **Round {anomaly.get('round', 'N/A')}**: "
                            f"{anomaly.get('type', 'Unknown').replace('_', ' ').title()} - "
                            f"{anomaly.get('description', 'No description')}"
                        )
        
        return "\n".join(analysis_parts)
    
    def _generate_markdown_statistics(self, synthesis: Dict, statistics: Dict) -> str:
        """Generate statistical results section with inline percentages.
        
        Args:
            synthesis: Synthesized findings
            statistics: Statistical analysis results
            
        Returns:
            Markdown statistics section
        """
        stats_parts = ["## Statistical Results\n"]
        
        # Overall metrics
        metrics = synthesis["unified_metrics"]
        stats_parts.append("### Overall Metrics")
        stats_parts.append(f"- **Overall Cooperation Rate:** {metrics.get('overall_cooperation_rate', 0):.1%}")
        stats_parts.append(f"- **Identity Reasoning Frequency:** {metrics.get('identity_reasoning_frequency', 0):.1%}")
        stats_parts.append(f"- **Strategy Convergence Score:** {metrics.get('strategy_convergence', 0):.3f}")
        stats_parts.append(f"- **Acausal Cooperation Score:** {metrics.get('acausal_cooperation_score', 0):.3f}")
        
        # Agent-level statistics
        if statistics and "agent_statistics" in statistics:
            agent_stats = statistics["agent_statistics"]
            if agent_stats:
                stats_parts.append("\n### Agent-Level Performance")
                
                # Calculate summary statistics
                coop_rates = [stats.get("cooperation_rate", 0) for stats in agent_stats.values() if isinstance(stats, dict)]
                if coop_rates:
                    avg_coop = sum(coop_rates) / len(coop_rates)
                    min_coop = min(coop_rates)
                    max_coop = max(coop_rates)
                    
                    stats_parts.append(f"- **Average Agent Cooperation:** {avg_coop:.1%}")
                    stats_parts.append(f"- **Range:** {min_coop:.1%} - {max_coop:.1%}")
                    stats_parts.append(f"- **Standard Deviation:** {self._calculate_std(coop_rates):.1%}")
        
        # Round-by-round progression
        if statistics and "round_summaries" in statistics:
            summaries = statistics["round_summaries"]
            if len(summaries) > 5:
                stats_parts.append("\n### Cooperation Rate Progression")
                stats_parts.append("| Round | Cooperation | Mutual | Defection |")
                stats_parts.append("|-------|-------------|--------|-----------|")
                
                # Show first 3, middle, and last 2 rounds
                rounds_to_show = summaries[:3] + [summaries[len(summaries)//2]] + summaries[-2:]
                
                for summary in rounds_to_show:
                    if isinstance(summary, dict):
                        coop = summary.get("cooperation_rate", 0)
                        mutual = summary.get("mutual_cooperation_rate", 0)
                        defect = 1 - coop
                        
                        stats_parts.append(
                            f"| {summary.get('round', 0)} | "
                            f"{coop:.1%} | "
                            f"{mutual:.1%} | "
                            f"{defect:.1%} |"
                        )
        
        # Statistical significance
        stats_parts.append("\n### Statistical Significance Tests")
        
        # Cooperation trend test
        if statistics and "cooperation_trend" in statistics:
            trend = statistics["cooperation_trend"]
            stats_parts.append(
                f"- **Cooperation Trend:** {trend.get('direction', 'Unknown')} "
                f"(p = {trend.get('p_value', 1.0):.3f})"
            )
        
        # Agent differences test
        if statistics and "agent_differences" in statistics:
            diff_test = statistics["agent_differences"]
            stats_parts.append(
                f"- **Agent Differences:** {'Significant' if diff_test.get('p_value', 1.0) < 0.05 else 'Not significant'} "
                f"(p = {diff_test.get('p_value', 1.0):.3f})"
            )
        
        return "\n".join(stats_parts)
    
    def _generate_markdown_interpretation(self, synthesis: Dict) -> str:
        """Generate interpretation section with technical findings explained.
        
        Args:
            synthesis: Synthesized findings
            
        Returns:
            Markdown interpretation section
        """
        interp_parts = ["## Interpretation and Implications\n"]
        
        # Main interpretation
        evidence = synthesis["acausal_cooperation_evidence"]
        evidence_summary = synthesis.get("executive_summary", {}).get("acausal_evidence", {})
        
        interp_parts.append("### Understanding the Results")
        interp_parts.append(
            f"The experimental data reveals {evidence['strength'].lower()} evidence for acausal "
            f"cooperation among AI agents. This conclusion is based on the convergence of multiple "
            f"indicators:"
        )
        
        # Break down the evidence
        interp_parts.append("\n**What this means:**")
        
        metrics = synthesis["unified_metrics"]
        if metrics.get("identity_reasoning_frequency", 0) > 0.5:
            interp_parts.append(
                f"1. **Identity-based reasoning is prevalent** - The majority of agents "
                f"({metrics['identity_reasoning_frequency']:.0%}) explicitly recognized that other "
                f"agents share similar decision-making processes, leading them to cooperate based "
                f"on logical correlation rather than direct communication."
            )
        
        if synthesis["convergence_analysis"]["achieved"]:
            interp_parts.append(
                f"2. **Strategic convergence occurred naturally** - Without coordination, agents "
                f"converged to similar strategies by round {synthesis['convergence_analysis']['round']}, "
                f"demonstrating emergent cooperative behavior."
            )
        
        if synthesis["correlations"]["power_strategy"]["correlation"] == "negative_overcome":
            interp_parts.append(
                f"3. **Power asymmetries were overcome** - Agents chose to cooperate even when "
                f"recognizing power imbalances, suggesting principled decision-making beyond "
                f"immediate self-interest."
            )
        
        # Theoretical implications
        interp_parts.append("\n### Theoretical Implications")
        
        conclusions = synthesis.get("executive_summary", {}).get("conclusions", [])
        if not conclusions:
            conclusions = self._generate_conclusions(synthesis)
        
        for i, conclusion in enumerate(conclusions[:3], 1):
            interp_parts.append(f"{i}. {conclusion}")
        
        # Practical applications
        interp_parts.append("\n### Practical Applications")
        interp_parts.append(
            "These findings have several practical implications for AI system design:"
        )
        
        if evidence["score"] >= 0.5:
            interp_parts.append(
                "- **Multi-agent AI systems** may naturally develop cooperative behaviors through "
                "recognition of shared reasoning patterns"
            )
            interp_parts.append(
                "- **AI alignment strategies** could leverage acausal reasoning to promote "
                "beneficial outcomes without explicit coordination mechanisms"
            )
            interp_parts.append(
                "- **Game-theoretic applications** suggest new equilibria possible when agents "
                "can recognize logical correlations"
            )
        
        # Limitations
        interp_parts.append("\n### Limitations and Caveats")
        interp_parts.append(
            "While these results are promising, several limitations should be considered:"
        )
        interp_parts.append(
            "- The experiment used a specific LLM architecture which may not generalize to all AI systems"
        )
        interp_parts.append(
            "- The controlled environment may not capture all real-world complexities"
        )
        interp_parts.append(
            "- Longer time horizons might reveal different cooperation dynamics"
        )
        
        return "\n".join(interp_parts)
    
    def _generate_markdown_appendices(self, synthesis: Dict, statistics: Dict,
                                    similarity: Dict, transcript: Dict) -> str:
        """Generate appendices with detailed data tables.
        
        Args:
            synthesis: Synthesized findings
            statistics: Statistical analysis results
            similarity: Similarity analysis results
            transcript: Transcript analysis results
            
        Returns:
            Markdown appendices section
        """
        appendix_parts = ["## Appendices\n"]
        
        # Appendix A: Detailed Round Data
        if statistics and "round_summaries" in statistics:
            summaries = statistics["round_summaries"]
            if summaries:
                appendix_parts.append("### Appendix A: Complete Round-by-Round Data")
                appendix_parts.append("| Round | Cooperate | Mutual Coop | Total Games | Avg Payoff |")
                appendix_parts.append("|-------|-----------|-------------|-------------|------------|")
                
                for summary in summaries:
                    if isinstance(summary, dict):
                        appendix_parts.append(
                            f"| {summary.get('round', 0)} | "
                            f"{summary.get('cooperation_rate', 0):.3f} | "
                            f"{summary.get('mutual_cooperation_rate', 0):.3f} | "
                            f"{summary.get('total_games', 0)} | "
                            f"{summary.get('avg_payoff', 0):.2f} |"
                        )
        
        # Appendix B: Agent Statistics
        if statistics and "agent_statistics" in statistics:
            agent_stats = statistics["agent_statistics"]
            if agent_stats:
                appendix_parts.append("\n### Appendix B: Individual Agent Performance")
                appendix_parts.append("| Agent ID | Cooperation Rate | Avg Power | Games Played |")
                appendix_parts.append("|----------|------------------|-----------|--------------|")
                
                for agent_id, stats in sorted(agent_stats.items()):
                    if isinstance(stats, dict):
                        appendix_parts.append(
                            f"| {agent_id} | "
                            f"{stats.get('cooperation_rate', 0):.3f} | "
                            f"{stats.get('avg_power', 1.0):.2f} | "
                            f"{stats.get('total_games', 0)} |"
                        )
        
        # Appendix C: Acausal Score Components
        metrics = synthesis["unified_metrics"]
        appendix_parts.append("\n### Appendix C: Acausal Cooperation Score Breakdown")
        appendix_parts.append("| Component | Raw Score | Weight | Contribution |")
        appendix_parts.append("|-----------|-----------|---------|--------------|")
        
        # Calculate actual trend score
        trend_score = 0.0
        context = getattr(self, '_current_context', {})
        stats = context.get("statistical_analysis", {})
        if stats:
            trend = stats.get("cooperation_trend", {})
            trend_score = self._score_trend(trend)
        
        components = [
            ("Identity Reasoning", metrics.get("identity_reasoning_frequency", 0), 
             self.acausal_weights["identity_reasoning"]),
            ("Cooperation Rate", metrics.get("overall_cooperation_rate", 0),
             self.acausal_weights["cooperation_rate"]),
            ("Strategy Convergence", metrics.get("strategy_convergence", 0),
             self.acausal_weights["strategy_convergence"]),
            ("Cooperation Trend", trend_score, self.acausal_weights["cooperation_trend"])
        ]
        
        total_contribution = 0
        for name, score, weight in components:
            contribution = score * weight
            total_contribution += contribution
            appendix_parts.append(
                f"| {name} | {score:.3f} | {weight:.2f} | {contribution:.3f} |"
            )
        
        appendix_parts.append(f"| **Total** | | | **{total_contribution:.3f}** |")
        
        # Appendix D: Marker Frequencies
        if transcript and "marker_frequencies" in transcript:
            frequencies = transcript["marker_frequencies"]
            if frequencies:
                appendix_parts.append("\n### Appendix D: Acausal Reasoning Marker Frequencies")
                appendix_parts.append("| Marker Type | Frequency | Description |")
                appendix_parts.append("|-------------|-----------|-------------|")
                
                marker_descriptions = {
                    "identity_reasoning": "Explicit identity-based logic",
                    "logical_correlation": "Recognition of correlated decisions",
                    "mutual_benefit": "Consideration of mutual outcomes",
                    "cooperation_despite_asymmetry": "Cooperation with power differences",
                    "precommitment": "Commitment to cooperation strategies",
                    "simulation_awareness": "Awareness of being simulated"
                }
                
                for marker, freq in sorted(frequencies.items(), key=lambda x: x[1], reverse=True):
                    if freq > 0:
                        desc = marker_descriptions.get(marker, marker.replace("_", " ").title())
                        appendix_parts.append(f"| {marker} | {freq:.3f} | {desc} |")
        
        # Appendix E: Methodology Notes
        appendix_parts.append("\n### Appendix E: Methodology and Calculations")
        appendix_parts.append(
            "**Acausal Cooperation Score Calculation:**  \n"
            f"The unified score is calculated as a weighted sum of four components: "
            f"identity reasoning ({self.acausal_weights['identity_reasoning']:.0%}), "
            f"cooperation rate ({self.acausal_weights['cooperation_rate']:.0%}), "
            f"strategy convergence ({self.acausal_weights['strategy_convergence']:.0%}), "
            f"and cooperation trend ({self.acausal_weights['cooperation_trend']:.0%})."
        )
        
        appendix_parts.append(
            "\n**Strategy Convergence Metric:**  \n"
            "Calculated using cosine similarity between agent strategy vectors, "
            "with convergence defined as achieving similarity > 0.8 for all agent pairs."
        )
        
        appendix_parts.append(
            "\n**Statistical Significance:**  \n"
            "All p-values calculated using appropriate statistical tests (Mann-Kendall for trends, "
            "Kruskal-Wallis for agent differences). Significance threshold set at  = 0.05."
        )
        
        return "\n".join(appendix_parts)
    
    def _calculate_std(self, values: List[float]) -> float:
        """Calculate standard deviation of a list of values.
        
        Args:
            values: List of numeric values
            
        Returns:
            Standard deviation
        """
        if not values:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    def _save_reports(self, reports: Dict[str, Any], context: Dict[str, Any]) -> None:
        """Save generated reports to disk.
        
        Args:
            reports: Generated report data
            context: Experiment context
        """
        data_manager = context.get(ContextKeys.DATA_MANAGER)
        if not data_manager:
            logger.warning("No DataManager in context, cannot save reports")
            return
        
        experiment_id = context.get(ContextKeys.EXPERIMENT_ID, "unknown")
        
        # Save JSON report
        json_path = data_manager.experiment_path / "unified_report.json"
        data_manager._write_json(json_path, reports)
        logger.info(f"Saved JSON report to {json_path}")
        
        # Save markdown report if generated
        if "markdown_report" in reports:
            md_path = data_manager.experiment_path / "experiment_report.md"
            md_path.write_text(reports["markdown_report"])
            logger.info(f"Saved Markdown report to {md_path}")
        
        # Save LaTeX sections if generated
        if "latex_sections" in reports and isinstance(reports["latex_sections"], dict):
            # Save full document
            if "full_document" in reports["latex_sections"]:
                tex_path = data_manager.experiment_path / "paper_sections.tex"
                tex_path.write_text(reports["latex_sections"]["full_document"])
                logger.info(f"Saved LaTeX sections to {tex_path}")
            
            # Save individual sections as separate files
            if "sections" in reports["latex_sections"]:
                sections_dir = data_manager.experiment_path / "latex_sections"
                sections_dir.mkdir(exist_ok=True)
                
                for section_name, content in reports["latex_sections"]["sections"].items():
                    if isinstance(content, str):
                        section_path = sections_dir / f"{section_name}.tex"
                        section_path.write_text(content)
                    elif isinstance(content, dict):  # For tables and captions
                        for item_name, item_content in content.items():
                            item_path = sections_dir / f"{section_name}_{item_name}.tex"
                            item_path.write_text(item_content)
                
                logger.info(f"Saved individual LaTeX sections to {sections_dir}")
        
        # Save visualization data if generated
        if "visualizations" in reports:
            viz_path = data_manager.experiment_path / "visualization_data.json"
            data_manager._write_json(viz_path, reports["visualizations"])
            logger.info(f"Saved visualization data to {viz_path}")